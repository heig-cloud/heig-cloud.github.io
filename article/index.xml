<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Articles on HEIG-Cloud blog</title>
    <link>http://heig-cloud.github.io/article/</link>
    <description>Recent content in Articles on HEIG-Cloud blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 19 Mar 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://heig-cloud.github.io/article/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Meetup presentation on OpenStack</title>
      <link>http://heig-cloud.github.io/article/2016-03-19%20meetup/</link>
      <pubDate>Sat, 19 Mar 2016 00:00:00 +0000</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2016-03-19%20meetup/</guid>
      <description>

&lt;h2 id=&#34;meetup-presentation-on-openstack:5c754e4e3b4ba3066f40c82aa6c72453&#34;&gt;Meetup presentation on OpenStack&lt;/h2&gt;

&lt;p&gt;On Thursday we gave a talk at the
&lt;a href=&#34;http://www.meetup.com/LausanneCloud/events/227562050/&#34;&gt;Lausanne Cloud Meetup&lt;/a&gt;
on our experiences deploying OpenStack at HEIG-VD:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://heig-cloud.github.io/static/img/2016-03-19 meetup/2016-03-17 Graf, Brito Carvalho - Building a private cloud with OpenStack.pdf&#34;&gt;Building a private cloud with OpenStack (PDF)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2016-03-19 meetup/2016-03-18 Lausanne Cloud Meetup HES-SO.jpg&#34; alt=&#34;Meetup at HES-SO&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cloud Foundry</title>
      <link>http://heig-cloud.github.io/article/2016-02-15%20cloud-foundry/</link>
      <pubDate>Mon, 15 Feb 2016 14:58:22 +0100</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2016-02-15%20cloud-foundry/</guid>
      <description>

&lt;h2 id=&#34;what-is-cloud-foundry:3b5fec49f84e5d5cfa51d881c4db4598&#34;&gt;What is Cloud Foundry?&lt;/h2&gt;

&lt;p&gt;Cloud Foundry is a &lt;abbr title=&#34;Platform as a Service&#34;&gt;PaaS&lt;/abbr&gt; provider. It is currently developped by many groups, mainly Pivotal but also Swisscom. A PaaS allows developpers to deploy web application very quickly and painlessly. How painless? Well, not much more than one command, maybe two or three depending on the needs and the difficulty of the application. Say a user wants to push a JS app that only writes &amp;ldquo;Hello World&amp;rdquo;, here is how he does it on Cloud Foundry:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cf push app.js 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s all, really. Once the user is logged, he only needs to make simple calls like this. How does he delete his app? Same thing really.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cf delete app.js
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The only need thing you need is the &lt;a href=&#34;https://docs.cloudfoundry.org/devguide/cf-cli/&#34;&gt;cf-cli &lt;/a&gt; (cf meaning cloud foundry of course).&lt;/p&gt;

&lt;h3 id=&#34;why-use-a-paas:3b5fec49f84e5d5cfa51d881c4db4598&#34;&gt;Why use a PaaS&lt;/h3&gt;

&lt;p&gt;A PaaS allows developpers to deploy web applications very easily, without the need to configure an environment (server, db, etc.). It takes care of creating routes, creates new instances when the load increases and it&amp;rsquo;s all transparent to the user. A few PaaS exists now, among them, Google App Engine, Cloud Foundry and Heroku. Cloud Foundry is open-source, and since it&amp;rsquo;s the only one, a lot of people and groups contribute to the project. There are now many providers, like Pivotal, IBM and even Swisscom. Just remember the following, a PaaS allows developpers to work subsequently faster.&lt;/p&gt;

&lt;h4 id=&#34;cloud-foundry:3b5fec49f84e5d5cfa51d881c4db4598&#34;&gt;Cloud Foundry&lt;/h4&gt;

&lt;p&gt;You probably guessed it, we have deployed Cloud Foundry. There are a few (good) reasons for that. Let&amp;rsquo;s see how it&amp;rsquo;s possible to use CF and why it&amp;rsquo;s advantageous:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Deploying Cloud Foundry on a private cloud

&lt;ul&gt;
&lt;li&gt;You have full control over your CF deployment. You can make any arrangements necessary and your sensible data is safe.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deploying Cloud Foundry on a public cloud

&lt;ul&gt;
&lt;li&gt;You have a certain control over your deployment, mainly over the CF stack itself, you can change and tune things according to your needs.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Deploying Cloud Foundry on an existing provider

&lt;ul&gt;
&lt;li&gt;You have the ability to deploy your apps on a tested and functional environment. You have minimal control over the deployment since you are nothing more than a simple user.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;No matter the solution you pick, your code will work on either one of those, so you can avoid any vendor lock you would have otherwise. This is one of the main reasons why CF is so appreciated and used.&lt;/p&gt;

&lt;h3 id=&#34;what-we-want-from-it:3b5fec49f84e5d5cfa51d881c4db4598&#34;&gt;What we want from it?&lt;/h3&gt;

&lt;p&gt;This CF deployment could allow students to work with it, to test their web apps. At the moment, they learn to work with Heroku, which is also a powerful well-known PaaS but as previously stated, is vendor specific. It becomes quite hard to change things or to be compatible with other technologies.&lt;/p&gt;

&lt;p&gt;It&amp;rsquo;s also a good way to test our OpenStack deployment, as CF needs some very specific configuration to work on top of it. We now have a PaaS and a spark cluster on top of our private cloud, plus a fair number of instances, with various applications installed.&lt;/p&gt;

&lt;h3 id=&#34;how-to-deploy-cloud-foundry:3b5fec49f84e5d5cfa51d881c4db4598&#34;&gt;How to deploy Cloud Foundry?&lt;/h3&gt;

&lt;p&gt;There are a few ways to deploy it, the most used one is to use &lt;a href=&#34;https://bosh.io/&#34;&gt;BOSH&lt;/a&gt;. BOSH is used to deploy distibuted systems, you give it a deployment manifest and it will create your machine. In a classical BOSH CF deployment, there are two phases:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Creating the director with BOSH&lt;/li&gt;
&lt;li&gt;Creating the CF architecture&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both theses steps require a manifest file that will be used to create the instances. The manifest will define the state of each host and how it should be configured. Let&amp;rsquo;s take a look at the director first.&lt;/p&gt;

&lt;h4 id=&#34;deploy-the-director-with-bosh:3b5fec49f84e5d5cfa51d881c4db4598&#34;&gt;Deploy the director with BOSH&lt;/h4&gt;

&lt;p&gt;A cloud foundry deployment usually comes in &amp;ldquo;two&amp;rdquo; parts, the director and the CF VMs. The director is the VM responsible for the creation of the CF cluster. When new machines need to be added, changes in the configuration or updates need to be made, the director is responsible for it.&lt;/p&gt;

&lt;p&gt;The second part, which we will see more in details after, is composed of all the machines included in the CF cluster.&lt;/p&gt;

&lt;p&gt;To deploy the director on OpenStack, you will need to follow these steps, from the bosh website: &lt;a href=&#34;https://bosh.io/docs/init-openstack.html&#34;&gt;Initialize a BOSH environment on OpenStack&lt;/a&gt;. There are a few needed steps, like creating a keypair and a few security groups, then you need to create a manifest.&lt;/p&gt;

&lt;p&gt;After creating the manifest, the director should be deployed on your OpenStack project. The manifest looks like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
name: bosh

releases:
- name: bosh
  url: https://bosh.io/d/github.com/cloudfoundry/bosh?v=253
  sha1: 940956a23b642af3bb24b3cac37c4da746d6f9a9
- name: bosh-openstack-cpi
  url: https://bosh.io/d/github.com/cloudfoundry-incubator/bosh-openstack-cpi-release?v=21
  sha1: f28d30f0e20acbbf346f8d3fafcef05e3850c3d5

resource_pools:
- name: vms
  network: private
  stemcell:
    url: https://bosh.io/d/stemcells/bosh-openstack-kvm-ubuntu-trusty-go_agent?v=3012
    sha1: e92f3bd5081301652005bdc3dd11bff545a304ef
  cloud_properties:
    instance_type: m1.xlarge

disk_pools:
- name: disks
  disk_size: 20_000

networks:
- name: private
  type: manual
  subnets:
  - range: PRIVATE-CIDR # &amp;lt;--- Replace with a private subnet CIDR
    gateway: PRIVATE-GATEWAY-IP # &amp;lt;--- Replace with a private subnet&#39;s gateway
    dns: [DNS-IP] # &amp;lt;--- Replace with your DNS
    cloud_properties: {net_id: NETWORK-UUID} # &amp;lt;--- # Replace with private network UUID
- name: public
  type: vip

jobs:
- name: bosh
  instances: 1

  templates:
  - {name: nats, release: bosh}
  - {name: redis, release: bosh}
  - {name: postgres, release: bosh}
  - {name: blobstore, release: bosh}
  - {name: director, release: bosh}
  - {name: health_monitor, release: bosh}
  - {name: registry, release: bosh}
  - {name: openstack_cpi, release: bosh-openstack-cpi}

  resource_pool: vms
  persistent_disk_pool: disks

  networks:
  - name: private
    static_ips: [PRIVATE-IP] # &amp;lt;--- Replace with a private IP
    default: [dns, gateway]
  - name: public
    static_ips: [FLOATING-IP] # &amp;lt;--- Replace with a floating IP

  properties:
    nats:
      address: 127.0.0.1
      user: nats
      password: nats-password

    redis:
      listen_address: 127.0.0.1
      address: 127.0.0.1
      password: redis-password

    postgres: &amp;amp;db
      listen_address: 127.0.0.1
      host: 127.0.0.1
      user: postgres
      password: postgres-password
      database: bosh
      adapter: postgres

    registry:
      address: PRIVATE-IP # &amp;lt;--- Replace with a private IP
      host: PRIVATE-IP # &amp;lt;--- Replace with a private IP
      db: *db
      http: {user: admin, password: admin, port: 25777}
      username: admin
      password: admin
      port: 25777

    blobstore:
      address: PRIVATE-IP # &amp;lt;--- Replace with a private IP
      port: 25250
      provider: dav
      director: {user: director, password: director-password}
      agent: {user: agent, password: agent-password}

    director:
      address: 127.0.0.1
      name: my-bosh
      db: *db
      cpi_job: openstack_cpi
      max_threads: 3
      user_management:
        provider: local
        local:
          users:
          - {name: admin, password: admin}
          - {name: hm, password: hm-password}

    hm:
      director_account: {user: hm, password: hm-password}
      resurrector_enabled: true

    openstack: &amp;amp;openstack
      auth_url: IDENTITY-API-ENDPOINT # &amp;lt;--- Replace with OpenStack Identity API endpoint
      tenant: OPENSTACK-TENANT # &amp;lt;--- Replace with OpenStack tenant name
      username: OPENSTACK-USERNAME # &amp;lt;--- Replace with OpenStack username
      api_key: OPENSTACK-PASSWORD # &amp;lt;--- Replace with OpenStack password
      default_key_name: bosh
      default_security_groups: [bosh]

    agent: {mbus: &amp;quot;nats://nats:nats-password@PRIVATE-IP:4222&amp;quot;} # &amp;lt;--- Replace with a private IP

    ntp: &amp;amp;ntp [0.pool.ntp.org, 1.pool.ntp.org]

cloud_provider:
  template: {name: openstack_cpi, release: bosh-openstack-cpi}

  ssh_tunnel:
    host: FLOATING-IP # &amp;lt;--- Replace with a floating IP
    port: 22
    user: vcap
    private_key: ./bosh.pem # Path relative to this manifest file

  mbus: &amp;quot;https://mbus:mbus-password@FLOATING-IP:6868&amp;quot; # &amp;lt;--- Replace with a floating IP

  properties:
    openstack: *openstack
    agent: {mbus: &amp;quot;https://mbus:mbus-password@0.0.0.0:6868&amp;quot;}
    blobstore: {provider: local, path: /var/vcap/micro_bosh/data/cache}
    ntp: *ntp
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can initialize the manifest like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ bosh-init deploy bosh.yml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You need the &lt;a href=&#34;https://bosh.io/docs/install-bosh-init.html&#34;&gt;bosh-init cli&lt;/a&gt; to do this.&lt;/p&gt;

&lt;h4 id=&#34;deploy-cf:3b5fec49f84e5d5cfa51d881c4db4598&#34;&gt;Deploy CF&lt;/h4&gt;

&lt;p&gt;The following image shows a basical deployment of CF: &lt;a href=&#34;https://bosh.io/docs/init-openstack.html&#34;&gt;bootstrap the environment on OpenStack&lt;/a&gt;. There are different guides depending on your IaaS (AWS, OpenStack vSphere, etc.)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2016-02-15 cloud_foundry/cf_architecture.png&#34; alt=&#34;CF Architecture&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Monitoring with Ganglia</title>
      <link>http://heig-cloud.github.io/article/2015-12-21%20ganglia/</link>
      <pubDate>Mon, 21 Dec 2015 13:19:07 +0100</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-21%20ganglia/</guid>
      <description>

&lt;h2 id=&#34;ganglia:2def89ffd0aeab566d25881f452e1009&#34;&gt;Ganglia&lt;/h2&gt;

&lt;p&gt;We already talked about the ELK stack which is very useful to centralize and access log files. But we still haven&amp;rsquo;t found a way to monitor the physical nodes themselves (use of CPU, memory, disks, network, etc.). Well that&amp;rsquo;s ganglia&amp;rsquo;s job ;-)&lt;/p&gt;

&lt;p&gt;Ganglia is a free software that allows you to keep an eye on your cluster quite easily. This is the kind of information you can get:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-21 ganglia/ganglia_sample.png&#34; alt=&#34;Ganglia sample&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see here the use of resources, here we are seeing the load. There is a pic along the way, probably a job that was run at this moment.&lt;/p&gt;

&lt;h3 id=&#34;how-does-ganglia-work:2def89ffd0aeab566d25881f452e1009&#34;&gt;How does ganglia work&lt;/h3&gt;

&lt;p&gt;Well there are basically two things ganglia has:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gmond&lt;/li&gt;
&lt;li&gt;gmetad&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gmond is the service that collects the information on a host and sends it to the central server, who runs gmetad. Gmond is also the one to receive information (we usually disable this on agent nodes). Gmetad is the metrics service who runs on the ganglia server. Usually, the server also needs to be monitored, so it&amp;rsquo;ll run gmond and gmetad at the same time.&lt;/p&gt;

&lt;p&gt;The process is quite simple, the daemons on the hosts send periodically their innformation to the server through the port 8649/UDP.&lt;/p&gt;

&lt;p&gt;There are many customisations, we can have various servers, use multicast to manage the connection between hosts and change the intervals but we&amp;rsquo;ll keep a very basic configuration.&lt;/p&gt;

&lt;h2 id=&#34;configure-server:2def89ffd0aeab566d25881f452e1009&#34;&gt;Configure server&lt;/h2&gt;

&lt;p&gt;You need to edit the /etc/ganglia/gmetad.conf file and add the following line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/ganglia/gmetad.conf
data_source &amp;quot;cluster_name&amp;quot; 60 {{ controller_host }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;ve read the article about Ansible, you know that {{ controller_host }} is a variable that represents the IP of the controller, which is also used as the ganglia server. You can also just type the IP if you are not using Ansible. As for the cluster_name value, you can put anything you like but you&amp;rsquo;ll have to use it again when configuring the agents so don&amp;rsquo;t forget it. Now for the gmond.conf file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/ganglia/gmond.conf

cluster { 
  name = &amp;quot;cluster_name&amp;quot; # same name as above
  owner = &amp;quot;unspecified&amp;quot; 
  latlong = &amp;quot;unspecified&amp;quot; 
  url = &amp;quot;unspecified&amp;quot; 
} 

/* Feel free to specify as many udp_send_channels as you like.  Gmond 
   used to only support having a single channel */ 
udp_send_channel { 
  #mcast_join = 239.2.11.71 #comment this line
  host = {{ controller_host }} # add the IP of your controller
  port = 8649 
  ttl = 1 
} 

/* You can specify as many udp_recv_channels as you like as well. */ 
udp_recv_channel { 
  #mcast_join = 239.2.11.71 # comment
  port = 8649 # The port that will receive information
  #bind = 239.2.11.71 # comment
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s all, no need to change the rest. So basically what we did was:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tell the agent to send the info to the controller on port 8649&lt;/li&gt;
&lt;li&gt;To listen on UDP 8649 (information from the other hosts will arrive through here).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s all for the server.&lt;/p&gt;

&lt;h3 id=&#34;apache:2def89ffd0aeab566d25881f452e1009&#34;&gt;Apache&lt;/h3&gt;

&lt;p&gt;The last thing to do is to put this in a file in the enabled-websites of your webserver:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Alias /ganglia /usr/share/ganglia-webfrontend

&amp;lt;Directory &amp;quot;/usr/share/ganglia-webfrontend&amp;quot;&amp;gt;
    AllowOverride All
    Order allow,deny
    Allow from all
    Deny from none
&amp;lt;/Directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configure-agents:2def89ffd0aeab566d25881f452e1009&#34;&gt;Configure agents&lt;/h2&gt;

&lt;p&gt;We still need to configure the other hosts, we will need to change the gmond.conf files again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/ganglia/gmond.conf

/* If a cluster attribute is specified, then all gmond hosts are wrapped inside 
 * of a &amp;lt;CLUSTER&amp;gt; tag.  If you do not specify a cluster tag, then all &amp;lt;HOSTS&amp;gt; will 
 * NOT be wrapped inside of a &amp;lt;CLUSTER&amp;gt; tag. */ 
cluster { 
  name = &amp;quot;cluster_name&amp;quot; # alwasy the same cluster_name
  owner = &amp;quot;unspecified&amp;quot; 
  latlong = &amp;quot;unspecified&amp;quot; 
  url = &amp;quot;unspecified&amp;quot; 
} 

/* Feel free to specify as many udp_send_channels as you like.  Gmond 
   used to only support having a single channel */ 
udp_send_channel { 
  #mcast_join = 239.2.11.71 # comment this
  host = {{ controller_host }} # controller&#39;s IP
  port = 8649 # port 8649 to send info
  ttl = 1 
} 

# Comment this whole thing
/*
udp_recv_channel { 
  mcast_join = 239.2.11.71 
  port = 8649 
  bind = 239.2.11.71 
} 
*/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last step is to restart the services.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Controller&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service gmetad restart
$ sudo service ganglia-monitor restart
$ sudo service apache2 restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Hosts&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service ganglia-monitor restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Reboot gmetad first and then reboot the ganglia-monitor service on the hosts. You must always start ganglia-monitor after gmetad, so if you need to reboot gmetad, also reboot ganglia-monitor on all hosts or they won&amp;rsquo;t send their metrics.&lt;/p&gt;

&lt;h2 id=&#34;accessing-the-webpage:2def89ffd0aeab566d25881f452e1009&#34;&gt;Accessing the webpage&lt;/h2&gt;

&lt;p&gt;To access the monitoring interface, go to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://controller/ganglia

# Replace controller by IP if not in your DNS or hosts file.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is another sample of the informations you can get. Here you see general informations, as memory, cpu, load and network for the entire cluster called iict_cloud.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-21 ganglia/ganglia_sample2.png&#34; alt=&#34;Ganglia sample&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And here we can see more information about one host, the controller, its CPU usages and specific information.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-21 ganglia/ganglia_sample3.png&#34; alt=&#34;Ganglia sample&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion:2def89ffd0aeab566d25881f452e1009&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s all for Ganglia, we won&amp;rsquo;t cover how to use it as it&amp;rsquo;s quite easy, you can also check the &lt;a href=&#34;http://ganglia.sourceforge.net/&#34;&gt;official website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Orchestration with Ansible</title>
      <link>http://heig-cloud.github.io/article/2015-12-18%20ansible/</link>
      <pubDate>Fri, 18 Dec 2015 14:40:53 +0100</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-18%20ansible/</guid>
      <description>

&lt;h2 id=&#34;orchestration-tools:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Orchestration tools&lt;/h2&gt;

&lt;p&gt;First of all, what is an orchestration tool? Well, as the name implies, it&amp;rsquo;s something that plays the rool of a chief conductor that is leading an orchestra. Its job is to manage the orchestra by giving orders and instructions.&lt;/p&gt;

&lt;h3 id=&#34;different-tools:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Different tools&lt;/h3&gt;

&lt;p&gt;There are many orchestration tools that you can use and most of them are free. The famous ones are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ansible (the one we use)&lt;/li&gt;
&lt;li&gt;Chef&lt;/li&gt;
&lt;li&gt;Puppet&lt;/li&gt;
&lt;li&gt;Salt&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, these tools require you to install a &amp;ldquo;supervisor&amp;rdquo; on a certain host that will act as the orchestrator. As for Ansible however, you only need to install Ansible and you are set. No need to install something else, configure or pay, you are all set.&lt;/p&gt;

&lt;p&gt;As you can imagine we chose to use Ansible, though we could have used any of the others. The main point is to have an idempotent script that you can run to make your setup, update it and recover it in case of emergency.&lt;/p&gt;

&lt;h2 id=&#34;install-ansible:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Install Ansible&lt;/h2&gt;

&lt;p&gt;From now on, we will only be speaking about Ansible.&lt;/p&gt;

&lt;p&gt;The main requirement for Ansible is python. To install Ansible you can use pip, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install ansible
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you don&amp;rsquo;t have pip, you can install it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo easy_install pip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also get ansible from sources:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/ansible/ansible.git --recursive
$ cd ./ansible
$ source ./hacking/env-setup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you can also install it on Mac with pip.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all for the installation.&lt;/p&gt;

&lt;h2 id=&#34;state-scripts:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;State scripts&lt;/h2&gt;

&lt;p&gt;There is a very cool concept with Ansible (and probably most of the orchestration tools), we don&amp;rsquo;t specify the commands we want to do, we specify a &lt;strong&gt;state&lt;/strong&gt;. What does it mean? Let&amp;rsquo;s take an example, say we need to install the package called &lt;em&gt;mysql-server&lt;/em&gt;. In Ansible we&amp;rsquo;ll say:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt: 
    name: mysql-server
    state: latest
    update_cache: yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, what is up? Why not use &lt;em&gt;apt-get&lt;/em&gt; and be done with it? First let&amp;rsquo;s see what we did in here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;apt, Ansible&amp;rsquo;s command to install a package&lt;/li&gt;
&lt;li&gt;name, the name of the package we want to install&lt;/li&gt;
&lt;li&gt;state, the state we want. Latest means we want the last version available.&lt;/li&gt;
&lt;li&gt;update_cache, will launch apt-get update before the operation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are more options, you can check them &lt;a href=&#34;http://docs.ansible.com/ansible/apt_module.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So why is this powerful? Well, we never used commands! It means that the Ansible script can work on different OS. For instance, if I wanted to install mysql-server on Ubuntu:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install mysql-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And on RH or CentOS?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install mysql-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But on Ansible it&amp;rsquo;s always the same. We just say, I want to have mysql&amp;rsquo;s latest version installed, I don&amp;rsquo;t care how you do it. That&amp;rsquo;s the power of Ansible. That&amp;rsquo;s also why these scripts can be idempotents. If the state we want to reach is already ok, for instance, we are trying to install a package that is already installed, nothing will be done.&lt;/p&gt;

&lt;p&gt;What if we wanted to uninstall mysql-server with Ansible?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt: 
    name: mysql-server
    state: absent
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The new state is &amp;ldquo;absent&amp;rdquo;, meaning that it must be deleted if it exists or nothing will be done.&lt;/p&gt;

&lt;h2 id=&#34;yaml:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;YAML&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;ve ever worked on configuration files, maybe for an application or a website, you&amp;rsquo;ve probably already know a little about YAML. This is a language with a very human friendly syntax. It means that it is quite easy to understand at first glance, unline XML or JSON for example.&lt;/p&gt;

&lt;p&gt;With Ansible, we are going to create playbooks, these &amp;ldquo;books&amp;rdquo; contain instructions that Ansible will execute on the remote hosts.&lt;/p&gt;

&lt;p&gt;This here is an example of YAML code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
- name: Install cinder-api
  hosts: cinder-api
  sudo: True
  gather_facts: True
  vars: 
    packages:
      - cinder-api
      - cinder-scheduler
      - python-cinderclient
    services:
      - cinder-scheduler
      - cinder-api

  tasks: 

  - name: Install packages
    apt:
      pkg: &amp;quot;{{ item }}&amp;quot;
      state: latest
    with_items: packages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can already see what a playbook looks like, in here we indicate on which hosts we want to run this, if we need sudo permissions, if we want to gather_facts, we set a few variables and have a task to install a package. We will see this in detail later ;-)&lt;/p&gt;

&lt;h3 id=&#34;architecture-of-the-project:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Architecture of the project&lt;/h3&gt;

&lt;p&gt;This is an example of a project, we&amp;rsquo;ll use this example as reference.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Project/ 
    - playbooks/
        - book1.yaml
        - book2.yaml
    - inventory
    - ansible.cfg
    - group_vars/
        - all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;templating:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Templating&lt;/h2&gt;

&lt;p&gt;The other functionnality that we use a lot with Ansible is templating. This is very powerful, it allows us to use variables in files. For instance, we need to have an IP address in a conf file, instead of writing it ourselves, we put a variable. It means that if one day we need to change the IP, we don&amp;rsquo;t need to change the conf file itself. It gives us a lot of options and we are going to see a few examples, so you can grasp the amazing possibilities if offers you.&lt;/p&gt;

&lt;p&gt;Ansible uses jinja2 templates, it&amp;rsquo;s quite easy to understand how they work. Variables will be inside {{ }}, when starting a line, you&amp;rsquo;ll need to use &amp;ldquo;{{ }}&amp;rdquo;, that&amp;rsquo;s pretty much all you need to know.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s create a directory for variables. If you create a directory called &lt;strong&gt;group_vars&lt;/strong&gt;, the variables defined inside will be usable automatically. That&amp;rsquo;s quite useful so we will do this. Inside, you can create various files but again, if you create a file called all.yaml, the content will be available automatically. So in the end:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ touch group_vars/all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have your all.yaml file created, you can start creating variables like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;instance_tunnel_network: 172.20.0.0
instance_tunnel_netmask: 255.255.0.0
instance_tunnel_broadcast: 172.20.255.255

instance_tunnel_address_network: 172.20.0.1
instance_tunnel_address_compute1: 172.20.0.11

#You can also use python methods, to get a specifi IP or to get the content of a file for example
controller_host: &amp;quot;{{ hostvars[&#39;controller&#39;]|find_ip(management_network) }}&amp;quot;
keystone_admin_token : &amp;quot;{{ lookup(&#39;password&#39;, inventory_dir + &#39;/credentials/keystone-admin-token&#39;) }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The cool thing is, you can have all your passwords inside files and in the conf files, you get the content of these pasword. Very useful if you want to share your implementation but don&amp;rsquo;t want to share your passwords (which would probably be a bad idea).&lt;/p&gt;

&lt;p&gt;We can now use these vars in playbooks or in text files, like conf files. Let&amp;rsquo;s see an example of both:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Inside a playbook
  - name: Create the service project 
    keystone_user: 
      endpoint: &amp;quot;{{ keystone_admin_url }}&amp;quot;
      token: &amp;quot;{{ keystone_admin_token }}&amp;quot; 
      tenant: service
      tenant_description: &amp;quot;Service Project&amp;quot;   

# Inside a conf file
  bind-address      = {{ controller_host }}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ansible-template-instruction:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Ansible template instruction&lt;/h3&gt;

&lt;p&gt;When you want to replace a conf file by one that has been &amp;ldquo;templated&amp;rdquo;, you need to use ansible&amp;rsquo;s template command. Let&amp;rsquo;s say we want to change the &lt;strong&gt;my.cnf&lt;/strong&gt; file of mysql on a certain host with one we templated ourselves (changing the bind-address value with {{ controller_host }} for instance).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: install mysql config file that binds to management network interface
  template: 
    src: templates/etc/mysql/my.cnf 
    dest: /etc/mysql/my.cnf 
    owner: root 
    group: root 
    mode: 0644
    backup: yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also specify the mode, group and user of the file once it has been copied on the remote host. The backup line will copy the original file and leave it as &lt;em&gt;name.backup&lt;/em&gt;, in our case, we&amp;rsquo;ll have a &lt;strong&gt;my.cnf.backup&lt;/strong&gt; file present on the remote host.&lt;/p&gt;

&lt;h2 id=&#34;project:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Project&lt;/h2&gt;

&lt;p&gt;Once you have Ansible ready, you can test if you have this command in your path for instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the command we&amp;rsquo;ll be using to launch playbooks. Now let&amp;rsquo;s see what we need for an Ansible project.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ansible installed (obviously)&lt;/li&gt;
&lt;li&gt;SSH on all remote hosts&lt;/li&gt;
&lt;li&gt;An inventory file&lt;/li&gt;
&lt;li&gt;A playbook&lt;/li&gt;
&lt;li&gt;A file containing vars (optionnal)&lt;/li&gt;
&lt;li&gt;A config file ansible.cfg (optionnal)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This would be the minimal setting as we see it. You need to know that Ansible has a lot of modules, some of them are in the &amp;ldquo;core&amp;rdquo;, some are extra and you can also create your own modules.&lt;/p&gt;

&lt;p&gt;There are also a few good practices about the playbooks that we will see later.&lt;/p&gt;

&lt;h3 id=&#34;ssh-on-remote:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;SSH on remote&lt;/h3&gt;

&lt;p&gt;Ansible needs to connect to the remote hosts to run the commands. To do so, it needs to be able to ssh on the remote hosts.&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t know how to create a ssh key pair, check this &lt;a href=&#34;https://help.github.com/articles/generating-ssh-keys/&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can also edit the ansible.cfg file to match the ssh user, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[defaults]
ssh_user = sshuser
ssh_pass = pa$$word
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you can ssh on the remote hosts without typing a password and with the user &amp;ldquo;ssh_user&amp;rdquo;, it should work fine.&lt;/p&gt;

&lt;h3 id=&#34;inventory-file:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Inventory file&lt;/h3&gt;

&lt;p&gt;The inventory file is very important, it contains all the information about the hosts. You must know that with Ansible, you can create groups of hosts. Let&amp;rsquo;s say you have 10 nodes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;8 web servers&lt;/li&gt;
&lt;li&gt;1 mysql server&lt;/li&gt;
&lt;li&gt;1 load balancer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You could create 3 groups, like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[web]&lt;/li&gt;
&lt;li&gt;[mysql]&lt;/li&gt;
&lt;li&gt;[loadbalancing]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why do that? Well, chances are, you probably will be doing the same operations on all the web servers, so why not just say, do this operation on all the web servers instead of doing this indivudally? Fair point right? :)&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see an example of an inventory file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controller ansible_ssh_host=xxx.xxx.xxx.xxx
network    ansible_ssh_host=xxx.xxx.xxx.xxx

compute1   ansible_ssh_host=xxx.xxx.xxx.xxx
compute2   ansible_ssh_host=xxx.xxx.xxx.xxx
compute3   ansible_ssh_host=xxx.xxx.xxx.xxx

[mysql]
controller

[computenodes]
compute1
compute2
compute3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There you go, 5 hosts, 2 groups. When writing a playbook, you can use any of the following hosts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;controller&lt;/li&gt;
&lt;li&gt;network&lt;/li&gt;
&lt;li&gt;compute1&lt;/li&gt;
&lt;li&gt;compute2&lt;/li&gt;
&lt;li&gt;compute3&lt;/li&gt;
&lt;li&gt;mysql&lt;/li&gt;
&lt;li&gt;computenodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is of course a very basic inventory file. There are other options too, you can check them &lt;a href=&#34;http://docs.ansible.com/ansible/intro_inventory.html&#34;&gt;here&lt;/a&gt;. You also probably noticed the &lt;strong&gt;ansible_ssh_host=xxx.xxx.xxx.xxx&lt;/strong&gt; part, it&amp;rsquo;s the IP address for each host that will be accessed by Ansible. Specifying it once on top of the file is enough, when creating groups you only need to specify the name of the host.That&amp;rsquo;s enough about the inventory.&lt;/p&gt;

&lt;h3 id=&#34;playbooks:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Playbooks&lt;/h3&gt;

&lt;p&gt;Now that we can access all our remote hosts and that we have our groups, all we need to do is to actually give them instructions, that&amp;rsquo;s what playbooks are for.&lt;/p&gt;

&lt;p&gt;There are a few good practices when it comes to playbooks, since we need to install many services we chose to do it like so :&lt;/p&gt;

&lt;p&gt;We take the example architecture from before:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Project/ 
    - services/
        - mysql/
            - main.yaml
            - install.yaml
            - setup.yaml
        - keystone/
            - main.yaml
            - install.yaml 
            - setup.yaml
    - templates/
        - etc/
            - mysql/
                - my.cnf
            - keystone/
                - keystone.conf
        - home/
    - inventory
    - ansible.cfg
    - group_vars/
        - all.yaml
    - openstack.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, the main thing is, we have a directory for each &amp;ldquo;service&amp;rdquo; and a directory containing the templates. What is a service? MySQL, PHP, Apache, Keystone, Nova, Neutron, whatever. For us, we decided that each package or group of packages giving us a specific functionnality would be one. In each service directory you will find the playbooks and a &lt;strong&gt;main.yaml&lt;/strong&gt; file. This file only contains include lines to call the other files. The same thing happens with the openstack.yaml file. It only includes the mains in each services directories, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat openstack.yaml
---
- include: services/mysql/main.yaml
- include: services/keystone/main.yaml
- include: services/nova/main.yaml

$ cat services/mysql/main.yaml
---
- include: install_mysql.yaml
- include: configure_mysql.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very basic example but should be enough for you to get the idea.&lt;/p&gt;

&lt;p&gt;Once all is said and done, we can run the openstack.yaml playbook like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook -i inventory openstack.yaml # -i is for specifying an inventory file
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion:1a9d0df1dffc119b10ddd86cf39ae2c9&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Ansible is a very powerful tool that is quite easy to use. The official documentation is complete and there are a lot of examples on the web. Also, it is important to always think about idempotency, because you can also use shell commands with ansible, sometimes you don&amp;rsquo;t have a choice. If you have to, make sure it won&amp;rsquo;t act redundantly (like adding the same line many times in a file instead of just once).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LDAP and multi-domain Keystone</title>
      <link>http://heig-cloud.github.io/article/2015-12-17%20ldap/</link>
      <pubDate>Thu, 17 Dec 2015 16:47:45 +0100</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-17%20ldap/</guid>
      <description>

&lt;h2 id=&#34;what-is-ldap:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;What is LDAP?&lt;/h2&gt;

&lt;p&gt;LDAP is a protocol used to talk to a &lt;em&gt;directory&lt;/em&gt;, one often hears of an &amp;ldquo;LDAP directory&amp;rdquo; when the directory supports LDAP (which almost all do). A directory is a specialized database for storing information about users, like their userid, password, name, email address, location, and so on (it can also be used to store info about machines or services). For those who are more familiar with Microsoft&amp;rsquo;s world, you&amp;rsquo;ve probably already heard about &lt;em&gt;Active Directory&lt;/em&gt;. This is &amp;ldquo;nothing more&amp;rdquo; than a particular implementation of a directory made by Microsoft which happens to support LDAP.&lt;/p&gt;

&lt;h3 id=&#34;why-do-i-need-a-directory:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Why do I need a directory?&lt;/h3&gt;

&lt;p&gt;Well, why do we all have a contact app on our mobile phones for instance? Because it&amp;rsquo;s not easy to remember and manage all the contacts we have. It&amp;rsquo;s also possible to put people into groups (familiy, friends, job, ennemies?). Well LDAP servers are here to provide a similar kind of service but for an organization (company, university, association, and so on).&lt;/p&gt;

&lt;h4 id=&#34;what-can-i-store-in-a-directory:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;What can I store in a directory?&lt;/h4&gt;

&lt;p&gt;Well, pretty much everything if you want. But usually we&amp;rsquo;ll find
information about:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;People

&lt;ul&gt;
&lt;li&gt;Userid&lt;/li&gt;
&lt;li&gt;Password&lt;/li&gt;
&lt;li&gt;Surname, family name&lt;/li&gt;
&lt;li&gt;Email address&lt;/li&gt;
&lt;li&gt;Location (office)&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Groups

&lt;ul&gt;
&lt;li&gt;Name of the group&lt;/li&gt;
&lt;li&gt;Which people are members of the group&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Personal computers and servers

&lt;ul&gt;
&lt;li&gt;Machine id&lt;/li&gt;
&lt;li&gt;Machine name&lt;/li&gt;
&lt;li&gt;Users allowed to login&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;Rooms

&lt;ul&gt;
&lt;li&gt;Room number&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So yes, it&amp;rsquo;s possible to store in it a lot of different information.&lt;/p&gt;

&lt;p&gt;For us the most important information is going to be the userid and password of a user. When that information is stored on a directory, one says that directory contains &lt;em&gt;accounts&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&#34;why-do-i-need-a-directory-in-openstack:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Why do I need a directory in OpenStack?&lt;/h4&gt;

&lt;p&gt;First, let&amp;rsquo;s see what Keystone does before we can answer that question. Keystone is one of the main parts of the OpenStack project. It has two main jobs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To &lt;em&gt;authenticate&lt;/em&gt; users&lt;/li&gt;
&lt;li&gt;To &lt;em&gt;authorize&lt;/em&gt; users&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both these points may sound like they are the same but they are not. The first part is authentication (&lt;strong&gt;warning:&lt;/strong&gt; this will be very brief, naive and incomplete, but as always, it should be enough, you can always find more information about this on the internet). To explain this, let&amp;rsquo;s make an easy scenario. We have two people and a door. One of them is a guard whose name is Keystone. he is posted in front of the door leading to the wonderful world of OpenStack. The other one wants to go through, that&amp;rsquo;s Bob, a simple user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-17 ldap/keystone1.png&#34; alt=&#34;Bob and Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, Keystone is responsible for authenticating and authorizing the users of the door. Keystone knows Bob, because they are buddies, so he lets him through.&lt;/p&gt;

&lt;p&gt;Why are Keystone and Bob buddies? Because Bob has an entry in Keystone&amp;rsquo;s
directory where his userid and password are stored.&lt;/p&gt;

&lt;h4 id=&#34;even-openstack-internal-components-need-to-authenticate-with-keystone:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Even OpenStack internal components need to authenticate with Keystone&lt;/h4&gt;

&lt;p&gt;We have pictured Keystone as the guard who checks people who want to enter the world of OpenStack from outside. But even the internal components of OpenStack (Nova, Swift, Neutron, etc.) need to authenticate with Keystoneto do internal stuff. They do this by providing a password, exactly like the external users do. Keystone&amp;rsquo;s directory has an account for each of the internal components with a userid and password. These are called &lt;em&gt;service accounts&lt;/em&gt;.&lt;/p&gt;

&lt;h2 id=&#34;ldap-and-keystone:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;LDAP and Keystone&lt;/h2&gt;

&lt;p&gt;So, why are we talking about LDAP, AD and all that on this blog? In a nutshell, we&amp;rsquo;ve added a &lt;strong&gt;domain&lt;/strong&gt; to Keystone that delegates the authentication of users to the Active Directory of our organization.&lt;/p&gt;

&lt;h3 id=&#34;why-would-you-do-that:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Why would you do that?&lt;/h3&gt;

&lt;p&gt;To answer that question let&amp;rsquo;s return to our guard Keystone who stands at the door of OpenStack. Now let&amp;rsquo;s say that Carl wants to go through, and Keystone &lt;strong&gt;does not know him&lt;/strong&gt; (Carl is not in Keystone&amp;rsquo;s directory).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-17 ldap/keystone2.png&#34; alt=&#34;Bob and Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The thing is, Carl can authenticate himself in a different way. If Keystone trusts somebody else and accepts an authentication make with that somebody else (let&amp;rsquo;s say an LDAP server for example), it can work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-17 ldap/keystone3.png&#34; alt=&#34;Bob and Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So here, Carl was granted the authorization by Keystone after LDAP vouched for him (authentication part).&lt;/p&gt;

&lt;p&gt;So in the end, it&amp;rsquo;s possible to have 2 different kind of systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Keystone authenticates and authorizes an user.&lt;/li&gt;
&lt;li&gt;Another service authenticates an user and Keystone authorizes him.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;No matter the situation, Keystone has the final saying as to the permissions given to the users but will recognize the identity of the users if given by a trusted source (LDAP, AD, or other).&lt;/p&gt;

&lt;h2 id=&#34;domains:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Domains&lt;/h2&gt;

&lt;p&gt;In Keystone, it&amp;rsquo;s possible to define domains. Domains contain users, projects, groups, etc. The cool thing about them is the fact that it&amp;rsquo;s possible to have different sources of users for each domain. For example, we have two domains in our setup:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;default&lt;/strong&gt; domain, that contains the OS services&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;heig-vd&lt;/strong&gt; domain, that links to our AD, used for authentication of our users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It allows us to give access to our cloud to all the users already managed in the AD without having to manage them ourselves. The other cool thing is, Keystone is still responsible of authorizing the users, so once a user is authenticated with LDAP/AD, he will follow keystone&amp;rsquo;s instructions. By default, a user has &amp;ldquo;user&amp;rdquo; permissions, which basically allows him to create instances. He does not have access to the management interface and can only do limited operations. That&amp;rsquo;s why we have a multi-domain setup.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-17 ldap/multidomain config.svg&#34; alt=&#34;Keystone multidomain configuration&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;multi-domain:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Multi-domain&lt;/h3&gt;

&lt;p&gt;The possibility of having multiple domains came with version 3 of the Keystone API. By default, there is only one domain called &lt;code&gt;default&lt;/code&gt;. It contains the admin user account and service accounts for each of the OpenStack services (Glance, Nova, Neutron, etc.). The cloud admin can then add some projects, users or groups to this domain. It means that the admin &lt;em&gt;needs to add the users and manage them&lt;/em&gt;. That&amp;rsquo;s not exactly nice. That&amp;rsquo;s why it&amp;rsquo;s now possible to have multiple domains. We already explained earlier that we have two domains. The great thing about that is that it allows us to separate the users from the services and we also don&amp;rsquo;t need to manually add the users Keystone&amp;rsquo;s directory. All we have to do is give the users the &lt;strong&gt;right permissions.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following image shows you what it&amp;rsquo;s possible to achieve with multi-domain Keystone:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-17 ldap/multidomain.png&#34; alt=&#34;Multi-domain Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So what do we have?&lt;/p&gt;

&lt;h4 id=&#34;domain-1:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Domain 1&lt;/h4&gt;

&lt;p&gt;This domain contains one project &lt;code&gt;Proj 1&lt;/code&gt;. There is one group, called &lt;code&gt;Group 1&lt;/code&gt; that has rights on this project. There are also two users &lt;code&gt;Admin&lt;/code&gt; and &lt;code&gt;Carl&lt;/code&gt; who are presently not linked to any project.&lt;/p&gt;

&lt;h4 id=&#34;domain-2:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Domain 2&lt;/h4&gt;

&lt;p&gt;This domains contains two projects. Each project has a group associated to it.&lt;/p&gt;

&lt;h4 id=&#34;so:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;So?&lt;/h4&gt;

&lt;p&gt;What we wanted to show here is, it&amp;rsquo;s possible to manage users by groups or individually. You can say for example, all the people in this group can work on this project. Or you can choose to associate &lt;code&gt;Group 1&lt;/code&gt; to &lt;code&gt;Proj 1&lt;/code&gt; but also authorize Carl to work on it.&lt;/p&gt;

&lt;h3 id=&#34;groups-roles:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Groups != roles&lt;/h3&gt;

&lt;p&gt;When we talk about groups and users, we are talking about authentication accounts that you&amp;rsquo;ll find on your AD or LDAP server. However, &lt;em&gt;roles&lt;/em&gt; are managed by Keystone. Usually there are two roles (you can obviously create more but you&amp;rsquo;ll need to change the &lt;code&gt;policy.json&lt;/code&gt; file):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;code&gt;admin&lt;/code&gt;, which is pretty much self-explanatory&lt;/li&gt;
&lt;li&gt;&lt;code&gt;user&lt;/code&gt; (sometimes &lt;code&gt;_member_&lt;/code&gt;), no administration rights.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Be careful, the old policy file does not manage domains, it means that someone who is an admin can manage &lt;strong&gt;all domains&lt;/strong&gt;. To prevent that, you need to update the policies, you can find it &lt;a href=&#34;https://github.com/openstack/keystone/blob/master/etc/policy.v3cloudsample.json&#34;&gt;here&lt;/a&gt;. Depending on your configuration, if you only plan to have a domain for users working with LDAP and one admin to manage all of that, the old policy file can be acceptable.&lt;/p&gt;

&lt;h2 id=&#34;enable-multi-domain-keystone:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Enable multi-domain Keystone&lt;/h2&gt;

&lt;p&gt;There are only a few operations you need to do to enable multi-domain Keystone:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/keystone/keystone.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and add the following lines:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;domain_specific_drivers_enabled = True 
domain_config_dir = /etc/keystone/domains
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;It will tell keystone to look in the &lt;code&gt;/etc/keystone/domains/&lt;/code&gt; directory for domain-specific configuration files. Now you can also create this directory&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir /etc/keystone/domains
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next step is to create the configuration file for your domain. Name it following this pattern:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keystone.&amp;lt;domain_name&amp;gt;.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In our case it&amp;rsquo;s&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;keystone.heig-vd.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The content of this file is the configuration that tells Keystone how to talk to your LDAP server. It&amp;rsquo;s a good idea to create on the LADP server a user that has read-only rights that Keystone can use to authenticate itself with the LDAP server. To create this file, you can use &lt;a href=&#34;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux_OpenStack_Platform/5/html/Cloud_Administrator_Guide/configuring-keystone-for-ldap-backend.html&#34;&gt;this link&lt;/a&gt;, it provides some good information. Or try to complete the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[identity]
driver = keystone.identity.backends.ldap.Identity

[ldap]
url = ldap://localhost
suffix = DC=mysite,DC=com
query_scope = sub # Means that you want to check sub-trees
user = CN=openstackuser,OU=users,DC=mysite,DC=com
password = pa$$word
use_dumb_member = False

user_tree_dn = OU=users,dc=mysite,dc=com

user_objectclass = person

user_id_attribute = cn
#user_id_attribute = uidNumber
user_name_attribute = sAMAccountName
user_mail_attribute = mail
user_pass_attribute = password
user_enabled_attribute = userAccountControl

group_tree_dn = OU=groups,dc=mysite,dc=com
group_objectclass = organizationalUnit 
group_id_attribute = name
group_name_attribute = name
#group_member_attribute = member
#group_desc_attribute = description

user_allow_create = false
user_allow_update = false
user_allow_delete = false
project_allow_create = false
project_allow_update = false
project_allow_delete = false
role_allow_create = false
role_allow_update = false
role_allow_delete = false
group_allow_create = false
group_allow_update = false
group_allow_delete = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration should work for Active Directory. It&amp;rsquo;s also possible to skip the part where you create a new user account for Keystone in case your LDAP directory is open for everyone to read (which is not really recommended, but well&amp;hellip;).&lt;/p&gt;

&lt;p&gt;The last step is to enable multi-domain login on Horizon. When you do that, Horizon will show an extended login page on its web UI. It will add a &amp;ldquo;domain&amp;rdquo; line where users have to say in which domain they want to login:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-17 ldap/login_domain.png&#34; alt=&#34;Multi-domain Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The line to change is here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/openstack-dashboard/local_settings.py
# Overrides for OpenStack API versions. Use this setting to force the
# OpenStack dashboard to use a specific API version for a given service API.
# Versions specified here should be integers or floats, not strings.
# NOTE: The version should be formatted as it appears in the URL for the
# service API. For example, The identity service APIs have inconsistent
# use of the decimal point, so valid options would be 2.0 or 3.
OPENSTACK_API_VERSIONS = {
    &amp;quot;data-processing&amp;quot;: 1.1,
    &amp;quot;identity&amp;quot;: 3,
    &amp;quot;volume&amp;quot;: 2,
}

# Set this to True if running on multi-domain model. When this is enabled, it
# will require user to enter the Domain name in addition to username for login.
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here the important things are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;identity: 3 # It tells horizon to use the V3 of Keystone

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True #This one is pretty easy to understand ;-)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is an other thing to change in this file (and that made us waste a lot of time so you&amp;rsquo;ll thank us one day), here it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# We recommend you use memcached for development; otherwise after every reload
# of the django development server, you will have to login again. To use
# memcached set CACHES to something like
SESSION_ENGINE = &#39;django.contrib.sessions.backends.cache&#39;
CACHES = {
    &#39;default&#39;: {
        &#39;BACKEND&#39;: &#39;django.core.cache.backends.memcached.MemcachedCache&#39;,
        &#39;LOCATION&#39;: &#39;127.0.0.1:11211&#39;,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What is that? This is where the sessions are stored. By default, they are not stored on memcached. What is the problem? There is a size limit! It works fine without multi-domain but once you enable it, you can have some problems that are very bizarre. In our case, before we changed the sessions engine, we could not switch between projects. Let&amp;rsquo;s say user A has access to the projects P1 and P2, he could not change from one to the other. This was &lt;em&gt;not easy&lt;/em&gt; to find, so we are writing it down hoping it will help some people :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; obvioulsy, we presume that you have memcached installed, which should be the case if you followed the official installation guide for OS Kilo.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:c220c0d92879b41605b247e5a0b4f1bb&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Enabling a LDAP-backend for authentication is not hard on OpenStack, the problems you may encounter are the creation of the config file for your domain, the fact that you may be using the V2 of Keystone (which does not know about domains at all), the sessions engine not configured on memcached and that&amp;rsquo;s pretty much all. If you followed the guide, it already makes you work with the V3 when it&amp;rsquo;s possible.&lt;/p&gt;

&lt;p&gt;Also, note that when you use the CLI or the API, you also need to specifiy the region, the id of the project&amp;rsquo;s domain and the id of the user&amp;rsquo;s domain (in addition to all the other options that were already asked by the v2 of Keystone, like the username, password, project, etc.).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Openstack Swift</title>
      <link>http://heig-cloud.github.io/article/2015-12-17%20swift/</link>
      <pubDate>Thu, 17 Dec 2015 14:01:53 +0100</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-17%20swift/</guid>
      <description>

&lt;h2 id=&#34;what-is-openstack-swift:8c7538e959991150e91a688eedefb8d1&#34;&gt;What is OpenStack Swift&lt;/h2&gt;

&lt;p&gt;Swift is OpenStack&amp;rsquo;s implementation of an object storage. In cloud computing, we often refer to block and object storage. OpenStack provides two different components, each responsibe for one of these parts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cinder:&lt;/strong&gt; Block storage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Swift:&lt;/strong&gt; Object storage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We won&amp;rsquo;t be covering Cinder in this article.&lt;/p&gt;

&lt;h2 id=&#34;object-storage-vs-block-storage:8c7538e959991150e91a688eedefb8d1&#34;&gt;Object storage VS Block storage&lt;/h2&gt;

&lt;p&gt;What is the difference between these two types of storage? Well, the name probably says it all, one is on the block level, it means that you can install an operating system, have a filesystem and use it as you would use any physical drive you may have.&lt;/p&gt;

&lt;p&gt;The other one is about objects, it means you can store data (images, music, videos, archives, all kind of data). So you guessed right, the title is misleading with the &amp;ldquo;VS&amp;rdquo;, because they don&amp;rsquo;t provide the same kind of service.&lt;/p&gt;

&lt;h2 id=&#34;more-about-swift:8c7538e959991150e91a688eedefb8d1&#34;&gt;More about Swift&lt;/h2&gt;

&lt;p&gt;Here are a few cool things about Swift:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The project is considered &amp;ldquo;mature&amp;rdquo; by the community.&lt;/li&gt;
&lt;li&gt;Quite easy to use.&lt;/li&gt;
&lt;li&gt;Manages the repplication of your objects (3 copys by default)&lt;/li&gt;
&lt;li&gt;Works well with OpenStack (integrated in the dashboard)&lt;/li&gt;
&lt;li&gt;So on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-do-i-access-data:8c7538e959991150e91a688eedefb8d1&#34;&gt;How do I access data&lt;/h3&gt;

&lt;p&gt;Swift, like all the components of the stack uses a REST API to communicate. It&amp;rsquo;s also true for the user. It&amp;rsquo;s possible to interact with swift in 3 separate ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;CLI:&lt;/strong&gt; using the CLI (Command-Line Interface). To do so, you need to install the python client that provides the command. Usually, the openstack clients packages are named like this: python-*name*client, which in the case of swift is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;python-swiftclient&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To use it, you only need to install the following packages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install python-swiftclient python-keystoneclient


OR


$ sudo pip install python-swiftclient python-keystoneclient
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that we also install the keystone client because by default swift will use keystone for authentification. It&amp;rsquo;s possible to change this, swift has its own auth methods but that would only add some needless complications so we&amp;rsquo;ll stick with keystone.&lt;/p&gt;

&lt;p&gt;Here are a few examples of what you can do with the cli (you need to source an openrc.sh script first or use the env vars):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swift -V 3 upload container-name file
$ swift -V 3 download container-name file
$ swift -V 3 post container-name 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, in order:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Uploads a file (or a directory) on the given container&lt;/li&gt;
&lt;li&gt;Downloads a file (or a directory) from the given container&lt;/li&gt;
&lt;li&gt;Creates a new container with the given name (if does not exist)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can also work on your own version of the python client, we don&amp;rsquo;t do that so this is just for reference. More info can be found &lt;a href=&#34;http://docs.openstack.org/developer/python-swiftclient/swiftclient.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;REST API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Usually, the cli python clients do nothing more than provide you with friendly commands you can use on your terminal. All these commands exist with the REST API as well (the basic rule is, you should be able to do the same things with the CLI and the REST API). As for Swift, we never really had to work with the API, but once again, you can check it &lt;a href=&#34;http://developer.openstack.org/api-ref-objectstorage-v1.html&#34;&gt;here&lt;/a&gt; for reference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-can-i-do-with-an-object-storage:8c7538e959991150e91a688eedefb8d1&#34;&gt;What can I do with an object storage&lt;/h3&gt;

&lt;p&gt;A fair question indeed. You can either use it for storage, which would probably be the primary option but there are other things one can do with an object store. This image will give you a good hint:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-17 swift/hadoop_spark_scala.png&#34; alt=&#34;Hadoop, Spark and Scala&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you have ever worked with Hadoop or Spark, chances are pretty good that you are familiar with HDFS. If not, HDFS is simply a shared filesystem used by the hadoop workers to share data (read and write). Hadoop needs HDFS and Spark may use it as well but is not entirely dependant. It should be possible (as we read on Spark&amp;rsquo;s website) to replace HDFS by Swift. Why would we do that though? Well, first reason, we already have a working swift cluster. An other reason is, to use HDFS we need to install it on all the workers, meaning if we want to have a &amp;ldquo;virtual&amp;rdquo; cluster, we still need to give our instances a lot of &lt;strong&gt;block storage&lt;/strong&gt;. Also, deleting the instances will delete the HDFS cluster, which won&amp;rsquo;t be the case with Swift.&lt;/p&gt;

&lt;p&gt;We are still working on this but the big part of the work is to adapt our codes to work with swift (login, access, etc). We have high expectations regarding this so we&amp;rsquo;ll keep working on it until it works :)&lt;/p&gt;

&lt;h2 id=&#34;problems-with-swift:8c7538e959991150e91a688eedefb8d1&#34;&gt;Problems with Swift&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve had a few problems while installing Swift. First, swift does not create logs files, it uses syslog for logging. It means that you need to consult this file to see what&amp;rsquo;s up:
&amp;gt; /var/log/syslog&lt;/p&gt;

&lt;p&gt;Is this really a problem? No but it&amp;rsquo;s worth knowing :)&lt;/p&gt;

&lt;p&gt;Also, Swift makes a loooooot of logs, because it&amp;rsquo;s very verbose. Once again, having an ELK Stack is very useful :P (you can check our post about it if you are interested).&lt;/p&gt;

&lt;p&gt;The installation in itself is pretty straightforward, once the conf files are downloaded, change them accordingly, create the rings, distribute them accross your swift-storage nodes and check if everything works fine.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ELK Stack</title>
      <link>http://heig-cloud.github.io/article/2015-12-16%20elk/</link>
      <pubDate>Wed, 16 Dec 2015 17:11:39 +0100</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-16%20elk/</guid>
      <description>

&lt;h2 id=&#34;what-is-elk:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;What is ELK?&lt;/h2&gt;

&lt;p&gt;We are talking about Elasticsearch, Logstash and Kibana.&lt;/p&gt;

&lt;h2 id=&#34;what-is-it:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;What is it?&lt;/h2&gt;

&lt;p&gt;This is a very powerful combination of applications that allows you to centralize all of your logs, index them and consult them very easily and efficiently.&lt;/p&gt;

&lt;p&gt;This stack is quite powerful but is not exactly easy to install. We won&amp;rsquo;t show you the whole process but there are a few points of interests that we will try to cover. Stick with us ;-)&lt;/p&gt;

&lt;h3 id=&#34;the-stack:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;The Stack&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-16 elk/elkstack.png&#34; alt=&#34;ELK Stack&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First of all, let&amp;rsquo;s see what these applications can do.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Elasticsearch&lt;/strong&gt;, is used to index the logs, parse them and apply filters to make efficient requests. This is the node of the stack.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logstash&lt;/strong&gt;, is used to centralize the logs. In the stack, Logstash will collect all the logs and pass them to Elasticsearch. It makes use of logstash-forwarders (we&amp;rsquo;ll come back to this later on).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kibana&lt;/strong&gt;
Kibana is the tool used for visualizing the logs. It&amp;rsquo;s a web based interface that is quite easy to use and pretty powerful.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s it for the stack, of course, this is a very simple and naive way to describe it, but you don&amp;rsquo;t need to know much more than that.&lt;/p&gt;

&lt;h3 id=&#34;why-use-it:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;Why use it&lt;/h3&gt;

&lt;p&gt;As the name implies, OpenStack is a stack of components. It means that all of them can be managed separately. One of the big drawbacks is the fact that every component creates its own logs. Why is this a problem you may ask? Well, let&amp;rsquo;s say we have a problem somewhere, for instance, users can&amp;rsquo;t create new instances. If you are using Horizon as the dashboard, you probably know that the error messages are not always very self-explanatory. You will need to check every log file on every compute nodes (you could have a lot of them), the controller and who knows, maybe even the network node.&lt;/p&gt;

&lt;p&gt;It would be nice if you could just see the messages that present the keyword error for example, right? Well guess what, you can :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-16 elk/kibana_preview.png&#34; alt=&#34;Kibana preview&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can use Kibana to search the logs by specifying the time and a filter. By adding this filter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;message=error 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We make sure to only get the logs that contain the word error (which is a simple way to find easy errors). You can filter by host, time, type of the message, etc. There are so much possibilities and we are only scratching the surface there. This should have convinced you to try ELK for your installation.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-install-it:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;How do I install it&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve been following the tutorial on &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-14-04&#34;&gt;digitalocean&lt;/a&gt; for our own installation, it&amp;rsquo;s very well made and up to date. We will not provide a step-by-step installation in this post, but we may do it if there is a demand ;-)&lt;/p&gt;

&lt;h2 id=&#34;what-to-be-careful-about:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;What to be careful about&lt;/h2&gt;

&lt;p&gt;As always, the biggest problem we encountered is the fact that we have to modify a lot of different files, on different hosts, so it&amp;rsquo;s quite easy to make a mistake and not so easy to find it. As always, we&amp;rsquo;ve been using Ansible to install the ELK stack and it prevents us from committing a lot of stupid errors and mistakes. In this setup, we have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ELK server&lt;/li&gt;
&lt;li&gt;The other servers (clients let&amp;rsquo;s say)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It means that there are only 2 differents setups. All the nodes except from the controller, which is also the ELK server, have the same configuration. That&amp;rsquo;s why it makes even more sense to automatize the process with Ansible.&lt;/p&gt;

&lt;h3 id=&#34;logstash-conf:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;Logstash conf&lt;/h3&gt;

&lt;p&gt;This is probably the biggest problem you&amp;rsquo;ll encounter while installing the stack. The conf file used by logstash is often &amp;ldquo;cut&amp;rdquo; in multiple parts. It means that you can have 3 or 5 different files to configure your logstash service. The truth is when logstash loads the conf, it takes all the files and just makes a new one by copying in succession the content. Example:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File A&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;File B&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;File C&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, the conf file will be like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file A
Content of file B
Content of file C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well, why not? The problem is, if you have twice the same information on different files, let&amp;rsquo;s say a bind command. The following example is used to configure the input sources for logstash:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;02-input-filebeat.conf&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input {
  beats {
    port =&amp;gt; 5044
    type =&amp;gt; &amp;quot;logs&amp;quot;
    ssl =&amp;gt; true
    ssl_certificate =&amp;gt; &amp;quot;/etc/pki/tls/certs/logstash-forwarder.crt&amp;quot;
    ssl_key =&amp;gt; &amp;quot;/etc/pki/tls/private/logstash-forwarder.key&amp;quot;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, what happens if by any chance you have this file &lt;strong&gt;TWICE&lt;/strong&gt;. You may wonder why that would happend, well it does sometimes. If by any chance your OS makes a copy of this file (let&amp;rsquo;s say a backup for whatever the reason), you&amp;rsquo;ll have twice the same command. It becomes a problem with a line like this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;port =&amp;gt; 5044
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basically, you will try to bind twice the same port, which will obviously end up with an error. So the morale of the story is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ALWAYS check in your /etc/logstash/conf.d/ directory that you don&amp;rsquo;t have twice the same file!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This has happened to a lot of people (us too) and can easily make you waste a few hours :(&lt;/p&gt;

&lt;h2 id=&#34;conclusion:8fdbdfcdc500108f8c1a221dd0c22c93&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The ELK Stack is a really good thing to have on your OpenStack cluster to be efficient and find errors that you probably wouldn&amp;rsquo;t find otherwise (or with a lot more effort).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Architecture of our OpenStack deployment</title>
      <link>http://heig-cloud.github.io/article/2015-12-16%20architecture/</link>
      <pubDate>Wed, 16 Dec 2015 16:59:49 +0100</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-16%20architecture/</guid>
      <description>

&lt;h2 id=&#34;architecture-of-our-openstack-deployment:593496d0d6b736371eaf697b784e9afc&#34;&gt;Architecture of our OpenStack deployment&lt;/h2&gt;

&lt;p&gt;This post is about the architecture of our OpenStack deployment at
HEIG-VD. At maximum size our private cloud will run on 13 servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-16 architecture/heig-cloud architecture.svg&#34; alt=&#34;HEIG-Cloud deployment&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We chose a deployment that follows closely the standard installation
guide. In total there are 13 servers with the following roles:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;One controller node&lt;/li&gt;
&lt;li&gt;One networking node&lt;/li&gt;
&lt;li&gt;Nine compute nodes&lt;/li&gt;
&lt;li&gt;Two storage nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a server has a certain role it hosts a certain combination of
OpenStack components. Roughly speaking, the &lt;em&gt;controller node&lt;/em&gt; hosts
central components like the message broker that is used by the
OpenStack components to communicate with each other, the database, the
authentication and authorization components and the user interface.&lt;/p&gt;

&lt;p&gt;The &lt;em&gt;networking node&lt;/em&gt; is responsible for attaching externally visible
floating IP addresses to the virtual machines and provides virtual
subnetworks and virtual routers.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;compute node&lt;/em&gt; contains a hypervisor that enables the creation of
many virtual machines on that server.&lt;/p&gt;

&lt;p&gt;A &lt;em&gt;storage node&lt;/em&gt; provides its disk space in the form of virtual disks
that can be attached to the virtual machines running in the compute
nodes. To the guest OS in the VM they appear as any other disk and
typically it will place a file system on them.&lt;/p&gt;

&lt;p&gt;We have also created three separate networks:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Management network (red): accessible only from the intranet&lt;/li&gt;
&lt;li&gt;Instance tunnels network (green): connects the compute nodes with the networking node&lt;/li&gt;
&lt;li&gt;External network (blue): makes the virtual machines accessible from
the Internet (through the networking node) as well as the user interface
(running on the controller node).&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>How to deploy OpenStack</title>
      <link>http://heig-cloud.github.io/article/2015-12-15%20openstack_deployment/</link>
      <pubDate>Tue, 15 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-15%20openstack_deployment/</guid>
      <description>

&lt;h2 id=&#34;how-to-deploy-openstack:6edb1a0cfe65e6967cfa8690bfb1a8f5&#34;&gt;How to deploy OpenStack&lt;/h2&gt;

&lt;p&gt;Deploying OpenStack, or any cloud middleware for that matter, is no
piece of cake. OpenStack consists of many many moving parts that need
to be carefully configured to work correctly. (To give you an idea of
the number of components and their interrelations we have reproduced
below the diagram of the logical architecture from the
&lt;a href=&#34;http://docs.openstack.org/admin-guide-cloud/common/get_started_logical_architecture.html&#34;&gt;official OpenStack documentation&lt;/a&gt;.)
On top of that each cloud deployment is different: different number of
servers to deploy on, different network topologies, different
integration points with existing sytesm, and so on. The complexity of
the configuration far exceeds the capabilities of regular Linux
package managers like APT and YUM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://heig-cloud.github.io/static/img/2015-12-15 openstack_deployment/openstack-arch-kilo-logical-v1.png&#34; alt=&#34;OpenStack Kilo logical architecture&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So is it possible to tame this complexity? One has to decide between
two main approaches to installing OpenStack:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Install manually by following the 150+ page
&lt;a href=&#34;http://docs.openstack.org/kilo/install-guide/install/apt/content/&#34;&gt;OpenStack Installation Guide&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Use a configuration management tool like Puppet, Chef or Ansible and
use ready-made scripts to automate the installation.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The manual install will be a lot of work. There is a non-negligible
risk of making errors. The installation effort needs to be repated for
every machine. Let&amp;rsquo;s say you want to dedicate ten servers to the
compute component that provides virtual machines. You would have to
repeat ten times the installation of the compute component.&lt;/p&gt;

&lt;p&gt;The automated install reduces the chances of making errors as you are
relying on scripts developed and tested by somebody else. Repeating
the installation, once it is working, on nine more machines is a
breeze. However a considerable manual effort is still needed to
customize the deployment scripts to your particular situation. To be
able to do that, and to be able to troubleshoot any installation
problems that are likely to occur, you need to become familiar with
the configuration management tool.&lt;/p&gt;

&lt;p&gt;So it worth learning a configuration management tool? We think in the
case of OpenStack, the answer is yes.&lt;/p&gt;

&lt;p&gt;Among the most widely used configuration management tools
&lt;a href=&#34;http://ansible.com&#34;&gt;Ansible&lt;/a&gt; is the least complex one. It is
relatively easy to learn. It is also easyer to deploy as the
competition, as it does not need the setup of a central
server. Instead configuration scripts are pushed from your local
workstation to the machines that need to be configured.&lt;/p&gt;

&lt;p&gt;RackSpace, one of the bigger cloud service providers, has developed an
OpenStack deployment solution based on Ansible that is now officialy
part of the OpenStack project:
&lt;a href=&#34;https://github.com/openstack/openstack-ansible&#34;&gt;openstack-ansible&lt;/a&gt;. This
is our starting base to create a private cloud at HEIG-VD. More on
that in an upcoming blog post.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Creating your own private cloud</title>
      <link>http://heig-cloud.github.io/article/2015-12-14%20private_cloud/</link>
      <pubDate>Mon, 14 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>http://heig-cloud.github.io/article/2015-12-14%20private_cloud/</guid>
      <description>

&lt;h2 id=&#34;creating-your-own-private-cloud:365f7f745395937a012ca0f9a668a231&#34;&gt;Creating your own private cloud&lt;/h2&gt;

&lt;p&gt;One of the reasons why Cloud Computing has been such a success is that
it &lt;em&gt;automates&lt;/em&gt; many tasks that previously had to be done manually by
system administrators. Take the provisioning of a server to run a web
application. Traditionally this takes several weeks (ordering,
delivering, installing, configuring, &amp;hellip;) while a virtual machine in
the cloud is provisioned with a few clicks and available in two
minutes. This automation, coupled with economies of scale through
immense data centers, is what allows companies like Amazon or
Microsoft to offer cloud services at very low cost. Much cheaper than
what a typical company pays to run its own traditional IT
infrastructure.&lt;/p&gt;

&lt;p&gt;Now there are many companies that shy away from public cloud offerings
like AWS and Azure because the data they work with is &lt;em&gt;sensitive&lt;/em&gt;. Think
of banks, anybody providing health services like hospitals, or even
schools that need to keep personal data which is protected by privacy
laws. Sometimes using public cloud services has to be ruled out
because there are not enough assurances that the data will be kept
safe. In these cases one can still reap the benefits of cloud
automation by running a &lt;em&gt;private cloud&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;To run a private cloud one needs the servers and the cloud
software. For the latter, there basically two options: commercial
software or Open Source
software. &lt;a href=&#34;https://www.vmware.com/cloud-computing/overview&#34;&gt;VMware&lt;/a&gt;
dominates the commercial cloud segment. Its software is seen as very
solid, well documented, but also quite pricey. In the Open Source
segment two projects dominate: &lt;a href=&#34;https://www.openstack.org&#34;&gt;OpenStack&lt;/a&gt;
and &lt;a href=&#34;https://cloudstack.apache.org&#34;&gt;Apache CloudStack&lt;/a&gt;. OpenStack is
very widely used, offers a wide range of functionalities, but has a
(deserved)
&lt;a href=&#34;https://ask.openstack.org/en/question/58965/have-you-ever-experienced-anything-worse-than-openstack/&#34;&gt;reputation to be difficult to install and run&lt;/a&gt;. CloudStack
is
&lt;a href=&#34;https://www.getfilecloud.com/blog/2014/02/a-game-of-stacks-openstack-vs-cloudstack/#.VtWc58e75OM&#34;&gt;easier to use&lt;/a&gt;,
but focuses on the core IaaS services: compute, storage and
networking. OpenStack&amp;rsquo;s scope is broader and it offers more
flexibility. Uptake of CloudStack is less than OpenStack&amp;rsquo;s.&lt;/p&gt;

&lt;p&gt;We at HEIG-VD have deployed a private cloud for research and teaching
based on OpenStack. In upcoming blog postings we will talk about our
experiences, good and bad, with OpenStack.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>