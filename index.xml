<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>HEIG-Cloud blog</title>
    <link>http://www.heig-cloud.github.io/</link>
    <description>Recent content on HEIG-Cloud blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 21 Dec 2015 13:19:07 +0100</lastBuildDate>
    <atom:link href="http://www.heig-cloud.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Monitoring with Ganglia</title>
      <link>http://www.heig-cloud.github.io/article/ganglia/</link>
      <pubDate>Mon, 21 Dec 2015 13:19:07 +0100</pubDate>
      
      <guid>http://www.heig-cloud.github.io/article/ganglia/</guid>
      <description>

&lt;h2 id=&#34;ganglia:154f68e092bb9cc4a3c5c5508681544a&#34;&gt;Ganglia&lt;/h2&gt;

&lt;p&gt;We already talked about the ELK stack which is very useful to centralize and access log files. But we still haven&amp;rsquo;t found a way to monitor the physical nodes themselves (use of CPU, memory, disks, network, etc.). Well that&amp;rsquo;s ganglia&amp;rsquo;s job ;-)&lt;/p&gt;

&lt;p&gt;Ganglia is a free software that allows you to keep an eye on your cluster quite easily. This is the kind of information you can get:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ganglia/ganglia_sample.png&#34; alt=&#34;Ganglia sample&#34; /&gt;&lt;/p&gt;

&lt;p&gt;You can see here the use of resources, here we are seeing the load. There is a pic along the way, probably a job that was run at this moment.&lt;/p&gt;

&lt;h3 id=&#34;how-does-ganglia-work:154f68e092bb9cc4a3c5c5508681544a&#34;&gt;How does ganglia work&lt;/h3&gt;

&lt;p&gt;Well there are basically two things ganglia has:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gmond&lt;/li&gt;
&lt;li&gt;gmetad&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Gmond is the service that collects the information on a host and sends it to the central server, who runs gmetad. Gmond is also the one to receive information (we usually disable this on agent nodes). Gmetad is the metrics service who runs on the ganglia server. Usually, the server also needs to be monitored, so it&amp;rsquo;ll run gmond and gmetad at the same time.&lt;/p&gt;

&lt;p&gt;The process is quite simple, the daemons on the hosts send periodically their innformation to the server through the port 8649/UDP.&lt;/p&gt;

&lt;p&gt;There are many customisations, we can have various servers, use multicast to manage the connection between hosts and change the intervals but we&amp;rsquo;ll keep a very basic configuration.&lt;/p&gt;

&lt;h2 id=&#34;configure-server:154f68e092bb9cc4a3c5c5508681544a&#34;&gt;Configure server&lt;/h2&gt;

&lt;p&gt;You need to edit the /etc/ganglia/gmetad.conf file and add the following line&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/ganglia/gmetad.conf
data_source &amp;quot;cluster_name&amp;quot; 60 {{ controller_host }}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you&amp;rsquo;ve read the article about Ansible, you know that {{ controller_host }} is a variable that represents the IP of the controller, which is also used as the ganglia server. You can also just type the IP if you are not using Ansible. As for the cluster_name value, you can put anything you like but you&amp;rsquo;ll have to use it again when configuring the agents so don&amp;rsquo;t forget it. Now for the gmond.conf file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/ganglia/gmond.conf

cluster { 
  name = &amp;quot;cluster_name&amp;quot; # same name as above
  owner = &amp;quot;unspecified&amp;quot; 
  latlong = &amp;quot;unspecified&amp;quot; 
  url = &amp;quot;unspecified&amp;quot; 
} 

/* Feel free to specify as many udp_send_channels as you like.  Gmond 
   used to only support having a single channel */ 
udp_send_channel { 
  #mcast_join = 239.2.11.71 #comment this line
  host = {{ controller_host }} # add the IP of your controller
  port = 8649 
  ttl = 1 
} 

/* You can specify as many udp_recv_channels as you like as well. */ 
udp_recv_channel { 
  #mcast_join = 239.2.11.71 # comment
  port = 8649 # The port that will receive information
  #bind = 239.2.11.71 # comment
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And that&amp;rsquo;s all, no need to change the rest. So basically what we did was:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Tell the agent to send the info to the controller on port 8649&lt;/li&gt;
&lt;li&gt;To listen on UDP 8649 (information from the other hosts will arrive through here).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s all for the server.&lt;/p&gt;

&lt;h3 id=&#34;apache:154f68e092bb9cc4a3c5c5508681544a&#34;&gt;Apache&lt;/h3&gt;

&lt;p&gt;The last thing to do is to put this in a file in the enabled-websites of your webserver:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Alias /ganglia /usr/share/ganglia-webfrontend

&amp;lt;Directory &amp;quot;/usr/share/ganglia-webfrontend&amp;quot;&amp;gt;
    AllowOverride All
    Order allow,deny
    Allow from all
    Deny from none
&amp;lt;/Directory&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;configure-agents:154f68e092bb9cc4a3c5c5508681544a&#34;&gt;Configure agents&lt;/h2&gt;

&lt;p&gt;We still need to configure the other hosts, we will need to change the gmond.conf files again.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/ganglia/gmond.conf

/* If a cluster attribute is specified, then all gmond hosts are wrapped inside 
 * of a &amp;lt;CLUSTER&amp;gt; tag.  If you do not specify a cluster tag, then all &amp;lt;HOSTS&amp;gt; will 
 * NOT be wrapped inside of a &amp;lt;CLUSTER&amp;gt; tag. */ 
cluster { 
  name = &amp;quot;cluster_name&amp;quot; # alwasy the same cluster_name
  owner = &amp;quot;unspecified&amp;quot; 
  latlong = &amp;quot;unspecified&amp;quot; 
  url = &amp;quot;unspecified&amp;quot; 
} 

/* Feel free to specify as many udp_send_channels as you like.  Gmond 
   used to only support having a single channel */ 
udp_send_channel { 
  #mcast_join = 239.2.11.71 # comment this
  host = {{ controller_host }} # controller&#39;s IP
  port = 8649 # port 8649 to send info
  ttl = 1 
} 

# Comment this whole thing
/*
udp_recv_channel { 
  mcast_join = 239.2.11.71 
  port = 8649 
  bind = 239.2.11.71 
} 
*/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Last step is to restart the services.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Controller&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service gmetad restart
$ sudo service ganglia-monitor restart
$ sudo service apache2 restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Hosts&lt;/strong&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo service ganglia-monitor restart
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; Reboot gmetad first and then reboot the ganglia-monitor service on the hosts. You must always start ganglia-monitor after gmetad, so if you need to reboot gmetad, also reboot ganglia-monitor on all hosts or they won&amp;rsquo;t send their metrics.&lt;/p&gt;

&lt;h2 id=&#34;accessing-the-webpage:154f68e092bb9cc4a3c5c5508681544a&#34;&gt;Accessing the webpage&lt;/h2&gt;

&lt;p&gt;To access the monitoring interface, go to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;http://controller/ganglia

# Replace controller by IP if not in your DNS or hosts file.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here is another sample of the informations you can get. Here you see general informations, as memory, cpu, load and network for the entire cluster called iict_cloud.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ganglia/ganglia_sample2.png&#34; alt=&#34;Ganglia sample&#34; /&gt;&lt;/p&gt;

&lt;p&gt;And here we can see more information about one host, the controller, its CPU usages and specific information.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ganglia/ganglia_sample3.png&#34; alt=&#34;Ganglia sample&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;conclusion:154f68e092bb9cc4a3c5c5508681544a&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;That&amp;rsquo;s all for Ganglia, we won&amp;rsquo;t cover how to use it as it&amp;rsquo;s quite easy, you can also check the &lt;a href=&#34;http://ganglia.sourceforge.net/&#34;&gt;official website&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Orchestration with Ansible</title>
      <link>http://www.heig-cloud.github.io/article/ansible/</link>
      <pubDate>Fri, 18 Dec 2015 14:40:53 +0100</pubDate>
      
      <guid>http://www.heig-cloud.github.io/article/ansible/</guid>
      <description>

&lt;h2 id=&#34;orchestration-tools:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Orchestration tools&lt;/h2&gt;

&lt;p&gt;First of all, what is an orchestration tool? Well, as the name implies, it&amp;rsquo;s something that plays the rool of a chief conductor that is leading an orchestra. Its job is to manage the orchestra by giving orders and instructions.&lt;/p&gt;

&lt;h3 id=&#34;different-tools:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Different tools&lt;/h3&gt;

&lt;p&gt;There are many orchestration tools that you can use and most of them are free. The famous ones are:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ansible (the one we use)&lt;/li&gt;
&lt;li&gt;Chef&lt;/li&gt;
&lt;li&gt;Puppet&lt;/li&gt;
&lt;li&gt;Salt&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Usually, these tools require you to install a &amp;ldquo;supervisor&amp;rdquo; on a certain host that will act as the orchestrator. As for Ansible however, you only need to install Ansible and you are set. No need to install something else, configure or pay, you are all set.&lt;/p&gt;

&lt;p&gt;As you can imagine we chose to use Ansible, though we could have used any of the others. The main point is to have an idempotent script that you can run to make your setup, update it and recover it in case of emergency.&lt;/p&gt;

&lt;h2 id=&#34;install-ansible:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Install Ansible&lt;/h2&gt;

&lt;p&gt;From now on, we will only be speaking about Ansible.&lt;/p&gt;

&lt;p&gt;The main requirement for Ansible is python. To install Ansible you can use pip, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo pip install ansible
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;If you don&amp;rsquo;t have pip, you can install it like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo easy_install pip
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also get ansible from sources:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ git clone git://github.com/ansible/ansible.git --recursive
$ cd ./ansible
$ source ./hacking/env-setup
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And you can also install it on Mac with pip.&lt;/p&gt;

&lt;p&gt;That&amp;rsquo;s all for the installation.&lt;/p&gt;

&lt;h2 id=&#34;state-scripts:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;State scripts&lt;/h2&gt;

&lt;p&gt;There is a very cool concept with Ansible (and probably most of the orchestration tools), we don&amp;rsquo;t specify the commands we want to do, we specify a &lt;strong&gt;state&lt;/strong&gt;. What does it mean? Let&amp;rsquo;s take an example, say we need to install the package called &lt;em&gt;mysql-server&lt;/em&gt;. In Ansible we&amp;rsquo;ll say:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt: 
    name: mysql-server
    state: latest
    update_cache: yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, what is up? Why not use &lt;em&gt;apt-get&lt;/em&gt; and be done with it? First let&amp;rsquo;s see what we did in here:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;apt, Ansible&amp;rsquo;s command to install a package&lt;/li&gt;
&lt;li&gt;name, the name of the package we want to install&lt;/li&gt;
&lt;li&gt;state, the state we want. Latest means we want the last version available.&lt;/li&gt;
&lt;li&gt;update_cache, will launch apt-get update before the operation.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are more options, you can check them &lt;a href=&#34;http://docs.ansible.com/ansible/apt_module.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So why is this powerful? Well, we never used commands! It means that the Ansible script can work on different OS. For instance, if I wanted to install mysql-server on Ubuntu:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install mysql-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;And on RH or CentOS?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ yum install mysql-server
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;But on Ansible it&amp;rsquo;s always the same. We just say, I want to have mysql&amp;rsquo;s latest version installed, I don&amp;rsquo;t care how you do it. That&amp;rsquo;s the power of Ansible. That&amp;rsquo;s also why these scripts can be idempotents. If the state we want to reach is already ok, for instance, we are trying to install a package that is already installed, nothing will be done.&lt;/p&gt;

&lt;p&gt;What if we wanted to uninstall mysql-server with Ansible?&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt: 
    name: mysql-server
    state: absent
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The new state is &amp;ldquo;absent&amp;rdquo;, meaning that it must be deleted if it exists or nothing will be done.&lt;/p&gt;

&lt;h2 id=&#34;yaml:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;YAML&lt;/h2&gt;

&lt;p&gt;If you&amp;rsquo;ve ever worked on configuration files, maybe for an application or a website, you&amp;rsquo;ve probably already know a little about YAML. This is a language with a very human friendly syntax. It means that it is quite easy to understand at first glance, unline XML or JSON for example.&lt;/p&gt;

&lt;p&gt;With Ansible, we are going to create playbooks, these &amp;ldquo;books&amp;rdquo; contain instructions that Ansible will execute on the remote hosts.&lt;/p&gt;

&lt;p&gt;This here is an example of YAML code:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;---
- name: Install cinder-api
  hosts: cinder-api
  sudo: True
  gather_facts: True
  vars: 
    packages:
      - cinder-api
      - cinder-scheduler
      - python-cinderclient
    services:
      - cinder-scheduler
      - cinder-api

  tasks: 

  - name: Install packages
    apt:
      pkg: &amp;quot;{{ item }}&amp;quot;
      state: latest
    with_items: packages
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can already see what a playbook looks like, in here we indicate on which hosts we want to run this, if we need sudo permissions, if we want to gather_facts, we set a few variables and have a task to install a package. We will see this in detail later ;-)&lt;/p&gt;

&lt;h3 id=&#34;architecture-of-the-project:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Architecture of the project&lt;/h3&gt;

&lt;p&gt;This is an example of a project, we&amp;rsquo;ll use this example as reference.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Project/ 
    - playbooks/
        - book1.yaml
        - book2.yaml
    - inventory
    - ansible.cfg
    - group_vars/
        - all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;templating:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Templating&lt;/h2&gt;

&lt;p&gt;The other functionnality that we use a lot with Ansible is templating. This is very powerful, it allows us to use variables in files. For instance, we need to have an IP address in a conf file, instead of writing it ourselves, we put a variable. It means that if one day we need to change the IP, we don&amp;rsquo;t need to change the conf file itself. It gives us a lot of options and we are going to see a few examples, so you can grasp the amazing possibilities if offers you.&lt;/p&gt;

&lt;p&gt;Ansible uses jinja2 templates, it&amp;rsquo;s quite easy to understand how they work. Variables will be inside {{ }}, when starting a line, you&amp;rsquo;ll need to use &amp;ldquo;{{ }}&amp;rdquo;, that&amp;rsquo;s pretty much all you need to know.&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s create a directory for variables. If you create a directory called &lt;strong&gt;group_vars&lt;/strong&gt;, the variables defined inside will be usable automatically. That&amp;rsquo;s quite useful so we will do this. Inside, you can create various files but again, if you create a file called all.yaml, the content will be available automatically. So in the end:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ touch group_vars/all.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you have your all.yaml file created, you can start creating variables like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;instance_tunnel_network: 172.20.0.0
instance_tunnel_netmask: 255.255.0.0
instance_tunnel_broadcast: 172.20.255.255

instance_tunnel_address_network: 172.20.0.1
instance_tunnel_address_compute1: 172.20.0.11

#You can also use python methods, to get a specifi IP or to get the content of a file for example
controller_host: &amp;quot;{{ hostvars[&#39;controller&#39;]|find_ip(management_network) }}&amp;quot;
keystone_admin_token : &amp;quot;{{ lookup(&#39;password&#39;, inventory_dir + &#39;/credentials/keystone-admin-token&#39;) }}&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The cool thing is, you can have all your passwords inside files and in the conf files, you get the content of these pasword. Very useful if you want to share your implementation but don&amp;rsquo;t want to share your passwords (which would probably be a bad idea).&lt;/p&gt;

&lt;p&gt;We can now use these vars in playbooks or in text files, like conf files. Let&amp;rsquo;s see an example of both:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Inside a playbook
  - name: Create the service project 
    keystone_user: 
      endpoint: &amp;quot;{{ keystone_admin_url }}&amp;quot;
      token: &amp;quot;{{ keystone_admin_token }}&amp;quot; 
      tenant: service
      tenant_description: &amp;quot;Service Project&amp;quot;   

# Inside a conf file
  bind-address      = {{ controller_host }}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;ansible-template-instruction:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Ansible template instruction&lt;/h3&gt;

&lt;p&gt;When you want to replace a conf file by one that has been &amp;ldquo;templated&amp;rdquo;, you need to use ansible&amp;rsquo;s template command. Let&amp;rsquo;s say we want to change the &lt;strong&gt;my.cnf&lt;/strong&gt; file of mysql on a certain host with one we templated ourselves (changing the bind-address value with {{ controller_host }} for instance).&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;- name: install mysql config file that binds to management network interface
  template: 
    src: templates/etc/mysql/my.cnf 
    dest: /etc/mysql/my.cnf 
    owner: root 
    group: root 
    mode: 0644
    backup: yes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You can also specify the mode, group and user of the file once it has been copied on the remote host. The backup line will copy the original file and leave it as &lt;em&gt;name.backup&lt;/em&gt;, in our case, we&amp;rsquo;ll have a &lt;strong&gt;my.cnf.backup&lt;/strong&gt; file present on the remote host.&lt;/p&gt;

&lt;h2 id=&#34;project:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Project&lt;/h2&gt;

&lt;p&gt;Once you have Ansible ready, you can test if you have this command in your path for instance:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ansible-playbook 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is the command we&amp;rsquo;ll be using to launch playbooks. Now let&amp;rsquo;s see what we need for an Ansible project.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Ansible installed (obviously)&lt;/li&gt;
&lt;li&gt;SSH on all remote hosts&lt;/li&gt;
&lt;li&gt;An inventory file&lt;/li&gt;
&lt;li&gt;A playbook&lt;/li&gt;
&lt;li&gt;A file containing vars (optionnal)&lt;/li&gt;
&lt;li&gt;A config file ansible.cfg (optionnal)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This would be the minimal setting as we see it. You need to know that Ansible has a lot of modules, some of them are in the &amp;ldquo;core&amp;rdquo;, some are extra and you can also create your own modules.&lt;/p&gt;

&lt;p&gt;There are also a few good practices about the playbooks that we will see later.&lt;/p&gt;

&lt;h3 id=&#34;ssh-on-remote:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;SSH on remote&lt;/h3&gt;

&lt;p&gt;Ansible needs to connect to the remote hosts to run the commands. To do so, it needs to be able to ssh on the remote hosts.&lt;/p&gt;

&lt;p&gt;If you don&amp;rsquo;t know how to create a ssh key pair, check this &lt;a href=&#34;https://help.github.com/articles/generating-ssh-keys/&#34;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;You can also edit the ansible.cfg file to match the ssh user, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[defaults]
ssh_user = sshuser
ssh_pass = pa$$word
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Once you can ssh on the remote hosts without typing a password and with the user &amp;ldquo;ssh_user&amp;rdquo;, it should work fine.&lt;/p&gt;

&lt;h3 id=&#34;inventory-file:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Inventory file&lt;/h3&gt;

&lt;p&gt;The inventory file is very important, it contains all the information about the hosts. You must know that with Ansible, you can create groups of hosts. Let&amp;rsquo;s say you have 10 nodes:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;8 web servers&lt;/li&gt;
&lt;li&gt;1 mysql server&lt;/li&gt;
&lt;li&gt;1 load balancer&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You could create 3 groups, like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;[web]&lt;/li&gt;
&lt;li&gt;[mysql]&lt;/li&gt;
&lt;li&gt;[loadbalancing]&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Why do that? Well, chances are, you probably will be doing the same operations on all the web servers, so why not just say, do this operation on all the web servers instead of doing this indivudally? Fair point right? :)&lt;/p&gt;

&lt;p&gt;Let&amp;rsquo;s see an example of an inventory file:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;controller ansible_ssh_host=xxx.xxx.xxx.xxx
network    ansible_ssh_host=xxx.xxx.xxx.xxx

compute1   ansible_ssh_host=xxx.xxx.xxx.xxx
compute2   ansible_ssh_host=xxx.xxx.xxx.xxx
compute3   ansible_ssh_host=xxx.xxx.xxx.xxx

[mysql]
controller

[computenodes]
compute1
compute2
compute3
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There you go, 5 hosts, 2 groups. When writing a playbook, you can use any of the following hosts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;controller&lt;/li&gt;
&lt;li&gt;network&lt;/li&gt;
&lt;li&gt;compute1&lt;/li&gt;
&lt;li&gt;compute2&lt;/li&gt;
&lt;li&gt;compute3&lt;/li&gt;
&lt;li&gt;mysql&lt;/li&gt;
&lt;li&gt;computenodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This is of course a very basic inventory file. There are other options too, you can check them &lt;a href=&#34;http://docs.ansible.com/ansible/intro_inventory.html&#34;&gt;here&lt;/a&gt;. You also probably noticed the &lt;strong&gt;ansible_ssh_host=xxx.xxx.xxx.xxx&lt;/strong&gt; part, it&amp;rsquo;s the IP address for each host that will be accessed by Ansible. Specifying it once on top of the file is enough, when creating groups you only need to specify the name of the host.That&amp;rsquo;s enough about the inventory.&lt;/p&gt;

&lt;h3 id=&#34;playbooks:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Playbooks&lt;/h3&gt;

&lt;p&gt;Now that we can access all our remote hosts and that we have our groups, all we need to do is to actually give them instructions, that&amp;rsquo;s what playbooks are for.&lt;/p&gt;

&lt;p&gt;There are a few good practices when it comes to playbooks, since we need to install many services we chose to do it like so :&lt;/p&gt;

&lt;p&gt;We take the example architecture from before:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Project/ 
    - services/
        - mysql/
            - main.yaml
            - install.yaml
            - setup.yaml
        - keystone/
            - main.yaml
            - install.yaml 
            - setup.yaml
    - templates/
        - etc/
            - mysql/
                - my.cnf
            - keystone/
                - keystone.conf
        - home/
    - inventory
    - ansible.cfg
    - group_vars/
        - all.yaml
    - openstack.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, the main thing is, we have a directory for each &amp;ldquo;service&amp;rdquo; and a directory containing the templates. What is a service? MySQL, PHP, Apache, Keystone, Nova, Neutron, whatever. For us, we decided that each package or group of packages giving us a specific functionnality would be one. In each service directory you will find the playbooks and a &lt;strong&gt;main.yaml&lt;/strong&gt; file. This file only contains include lines to call the other files. The same thing happens with the openstack.yaml file. It only includes the mains in each services directories, like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ cat openstack.yaml
---
- include: services/mysql/main.yaml
- include: services/keystone/main.yaml
- include: services/nova/main.yaml

$ cat services/mysql/main.yaml
---
- include: install_mysql.yaml
- include: configure_mysql.yaml
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This is a very basic example but should be enough for you to get the idea.&lt;/p&gt;

&lt;p&gt;Once all is said and done, we can run the openstack.yaml playbook like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ ansible-playbook -i inventory openstack.yaml # -i is for specifying an inventory file
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;conclusion:ad79c06bee1a26d7bf97bc259037a4ce&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Ansible is a very powerful tool that is quite easy to use. The official documentation is complete and there are a lot of examples on the web. Also, it is important to always think about idempotency, because you can also use shell commands with ansible, sometimes you don&amp;rsquo;t have a choice. If you have to, make sure it won&amp;rsquo;t act redundantly (like adding the same line many times in a file instead of just once).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LDAP and multi-domain Keystone</title>
      <link>http://www.heig-cloud.github.io/article/ldap/</link>
      <pubDate>Thu, 17 Dec 2015 16:47:45 +0100</pubDate>
      
      <guid>http://www.heig-cloud.github.io/article/ldap/</guid>
      <description>

&lt;h2 id=&#34;what-is-ldap:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;What is LDAP&lt;/h2&gt;

&lt;p&gt;LDAP is a protocole used to create a directory, we often speak of LDAP directories. For those who are more familiar with Microsoft&amp;rsquo;s world, you&amp;rsquo;ve probably already heard about Active Directory. This is &amp;ldquo;nothing more&amp;rdquo; than a special implementation of LDAP made by Microsoft. Of course that&amp;rsquo;s not true, but it&amp;rsquo;s possible to get both to work together. It&amp;rsquo;s not the aim of this post though ;-) (Even if we use Active Directory, or AD for short, and not another LDAP implementation).&lt;/p&gt;

&lt;h3 id=&#34;why-do-i-need-a-directory:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Why do I need a directory&lt;/h3&gt;

&lt;p&gt;Well, why do we all have an annuary on our celle phones for instance? Because it&amp;rsquo;s not easy to remember and manage all the contacts we have. It&amp;rsquo;s also possible to groupe people (familiy, friends, job, ennemies?). Well LDAP servers are here to provide that same kind of service but for an institution.&lt;/p&gt;

&lt;h4 id=&#34;what-can-i-store-in-a-directory:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;What can I store in a directory&lt;/h4&gt;

&lt;p&gt;Well, pretty much everything to be honest. Usually we&amp;rsquo;ll find:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;People&lt;/li&gt;
&lt;li&gt;Groups&lt;/li&gt;
&lt;li&gt;Personnal computers&lt;/li&gt;
&lt;li&gt;Servers&lt;/li&gt;
&lt;li&gt;Rooms?&lt;/li&gt;
&lt;li&gt;&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;So yes, it&amp;rsquo;s possible to store in a lot of information.&lt;/p&gt;

&lt;h2 id=&#34;ldap-and-keystone:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;LDAP and Keystone&lt;/h2&gt;

&lt;p&gt;So, why are we talking about LDAP, AD and all that on this blog? Quite simple, we&amp;rsquo;ve added a &lt;strong&gt;domain&lt;/strong&gt; that makes use of AD to authentificate users.&lt;/p&gt;

&lt;h3 id=&#34;why-do-that:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Why do that&lt;/h3&gt;

&lt;p&gt;First, let&amp;rsquo;s see what Keystone does before we can answer that question. Keystone is one of the main parts of the OpenStack project. It has two main jobs:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;To &lt;strong&gt;authentificate&lt;/strong&gt; users&lt;/li&gt;
&lt;li&gt;To &lt;strong&gt;authorize&lt;/strong&gt; users&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Both these points may sound like they are the same but they are not. The first part is authentification (&lt;strong&gt;warning:&lt;/strong&gt; this will be very brief, naive and incomplete, but as always, it should be enough, you can always find more information about this on the internet). To explain this, let&amp;rsquo;s make an easy scenario. We have two people and a door. One of them is a guard whose name is Keystone. he is posted in front of the door leading to the wonderful world of OpenStack. The other one wants to go through, that&amp;rsquo;s Bob, a simple user.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ldap/keystone1.png&#34; alt=&#34;Bob and Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;In this case, Keystone is responsible for authentificating and authorizing the users of the door. Keystone knows Bob, because they are buddies, so he lets him through.&lt;/p&gt;

&lt;p&gt;Now let&amp;rsquo;s say that Carl wants to go through, Keystone does not know him.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ldap/keystone2.png&#34; alt=&#34;Bob and Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The thing is, Carl can authentificate himslef differently. If keystone accepts authentification from that other medium (let&amp;rsquo;s say a LDAP server for example), it can work.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ldap/keystone3.png&#34; alt=&#34;Bob and Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So here, Carl was granted the authorization by Keystone after LDAP vouched for him (authentification part).&lt;/p&gt;

&lt;p&gt;So in the end, it&amp;rsquo;s possible to have 2 different kind of systems:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Keystone authentificates and authorizes an user.&lt;/li&gt;
&lt;li&gt;A service authentificate an user and Keystone authorizes him.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;No matter the situation, Keystone has the final saying as to the permissions given to the users but will recognize the identity of the users if given by a trusted source (LDAP, AD, or other).&lt;/p&gt;

&lt;h2 id=&#34;domains:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Domains&lt;/h2&gt;

&lt;p&gt;In Keystone, it&amp;rsquo;s possible to define domains. Domains contain users, projects, groups, etc. The cool thing about them is the fact that it&amp;rsquo;s possible to have different sources of users for each domain. For example, we have two domains in our setup:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;default&lt;/strong&gt; domain, that contains the OS services&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;heig-vd&lt;/strong&gt; domain, that links to our AD, used for authentification of our users.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It allows us to give access to our cloud to all the users already managed in the AD without having to manage them ourselves. The other cool thing is, Keystone is still responsible of authorizing the users, so once a user is authentificated with LDAP/AD, he will follow keystone&amp;rsquo;s instructions. By default, a user has &amp;ldquo;user&amp;rdquo; permissions, which basically allows him to create instances. He does not have access to the management interface and can only do limited operations. That&amp;rsquo;s why we have a multi-domain setup.&lt;/p&gt;

&lt;h3 id=&#34;multi-domain:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Multi-domain&lt;/h3&gt;

&lt;p&gt;The possibility of having multiple domains came with the v3 of the Keystone API. By default, there is only one domain called &amp;ldquo;default&amp;rdquo;. It contains the admin user and the services (glance, nova, neutron, etc.). The cloud admin can then add some projects, users or groups to this domain. It means that the admin &lt;em&gt;needs to add the users and manage them&lt;/em&gt;. That&amp;rsquo;s not exactly nice. That&amp;rsquo;s why it&amp;rsquo;s now possible to have multiple domains. We already explained earlier that we have 2 domains earlier, the truth is we can separate the users from the services and we also don&amp;rsquo;t need to manually add the users to the authentification pool. All we have to do is give the users the &lt;strong&gt;right permissions.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The following image shows you what it&amp;rsquo;s possible to achieve with multi-domain Keystone:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ldap/multidomain.png&#34; alt=&#34;Multi-domain Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;So what do we have?&lt;/p&gt;

&lt;h4 id=&#34;domain-1:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Domain 1&lt;/h4&gt;

&lt;p&gt;This domain contains 1 project. There is one group, called group 1 that has rights on this project. There are also 2 users who are presently not linked to any project.&lt;/p&gt;

&lt;h4 id=&#34;domain-2:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Domain 2&lt;/h4&gt;

&lt;p&gt;This domains contains 2 projects. Each project has a group associated.&lt;/p&gt;

&lt;h4 id=&#34;so:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;So?&lt;/h4&gt;

&lt;p&gt;What we wanted to show here is, it&amp;rsquo;s possible to manage users by groups or individually. You can say, all the people in this group can work on this project or for instance chose to associate group 1 to the proj 1 but also authorize Carl to work on it.&lt;/p&gt;

&lt;h3 id=&#34;groups-roles:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Groups != roles&lt;/h3&gt;

&lt;p&gt;When we talk about groups and users, we are talking about authentification accounts that you&amp;rsquo;ll find on your AD or LDAP server. However, Roles are managed by Keystone. Usually there are 2 roles (you can obviously create more but you&amp;rsquo;ll need to change the policy.json file):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;admin, which is pretty much self-explanatory&lt;/li&gt;
&lt;li&gt;user (sometimes _member_), no administration rights.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Be careful, the old policy file does not manage domains, it means that someone who is an admin can manage &lt;strong&gt;all domains&lt;/strong&gt;. To prevent that, you need to update the policies, you can find it &lt;a href=&#34;https://github.com/openstack/keystone/blob/master/etc/policy.v3cloudsample.json&#34;&gt;here&lt;/a&gt;. Depending on your configuration, if you only plan to have a domain for users working with LDAP and one admin to manage all of that, the old policy file can be acceptable.&lt;/p&gt;

&lt;h2 id=&#34;enable-multi-domain-keystone:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Enable multi-domain Keystone&lt;/h2&gt;

&lt;p&gt;There are only a few operations you need to do to enable multi-domain Keystone:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/keystone/keystone.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;and add the following lines:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;domain_specific_drivers_enabled = True&lt;/p&gt;

&lt;p&gt;domain_config_dir = /etc/keystone/domains&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It will tell keystone to look in the /etc/keystone/domains/ directory for domain-specific conf files. Now you can also create this directory&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo mkdir /etc/keystone/domains
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Next step is to create the conf file for your domain. Name it like:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;keystone.domain_name.conf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In our case it&amp;rsquo;s
&amp;gt; keystone.heig-vd.conf&lt;/p&gt;

&lt;p&gt;The content of this file is the configuration of your LDAP server. It&amp;rsquo;s a good idea to create an user that has read-only rights on the LDAP server that you&amp;rsquo;ll use for your authentification process. To create this file, you can use &lt;a href=&#34;https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux_OpenStack_Platform/5/html/Cloud_Administrator_Guide/configuring-keystone-for-ldap-backend.html&#34;&gt;this link&lt;/a&gt;, it provides some good information or try to complete the following:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[identity]
driver = keystone.identity.backends.ldap.Identity

[ldap]
url = ldap://localhost
suffix = DC=mysite,DC=com
query_scope = sub # Means that you want to check sub-trees
user = CN=openstackuser,OU=users,DC=mysite,DC=com
password = pa$$word
use_dumb_member = False

user_tree_dn = OU=users,dc=mysite,dc=com

user_objectclass = person

user_id_attribute = cn
#user_id_attribute = uidNumber
user_name_attribute = sAMAccountName
user_mail_attribute = mail
user_pass_attribute = password
user_enabled_attribute = userAccountControl

group_tree_dn = OU=groups,dc=mysite,dc=com
group_objectclass = organizationalUnit 
group_id_attribute = name
group_name_attribute = name
#group_member_attribute = member
#group_desc_attribute = description

user_allow_create = false
user_allow_update = false
user_allow_delete = false
project_allow_create = false
project_allow_update = false
project_allow_delete = false
role_allow_create = false
role_allow_update = false
role_allow_delete = false
group_allow_create = false
group_allow_update = false
group_allow_delete = false
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This configuration should work for Active Directory. It&amp;rsquo;s also possible to not use an user if you LDAP directory is open (which is not really good but well&amp;hellip;).&lt;/p&gt;

&lt;p&gt;The last step is to enable multi-domain login on horizon, it will add a &amp;ldquo;domain&amp;rdquo; line that you&amp;rsquo;ll have to fill when logging on the dashboard.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/ldap/login_domain.png&#34; alt=&#34;Multi-domain Keystone&#34; /&gt;&lt;/p&gt;

&lt;p&gt;The line to change is here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo nano /etc/openstack-dashboard/local_settings.py
# Overrides for OpenStack API versions. Use this setting to force the
# OpenStack dashboard to use a specific API version for a given service API.
# Versions specified here should be integers or floats, not strings.
# NOTE: The version should be formatted as it appears in the URL for the
# service API. For example, The identity service APIs have inconsistent
# use of the decimal point, so valid options would be 2.0 or 3.
OPENSTACK_API_VERSIONS = {
    &amp;quot;data-processing&amp;quot;: 1.1,
    &amp;quot;identity&amp;quot;: 3,
    &amp;quot;volume&amp;quot;: 2,
}

# Set this to True if running on multi-domain model. When this is enabled, it
# will require user to enter the Domain name in addition to username for login.
OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Here the important things are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;identity: 3 # It tells horizon to use the V3 of Keystone

OPENSTACK_KEYSTONE_MULTIDOMAIN_SUPPORT = True #This one is pretty easy to understand ;-)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There is an other thing to change in this file (and that made us waste a lot of time so you&amp;rsquo;ll thank us one day), here it is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# We recommend you use memcached for development; otherwise after every reload
# of the django development server, you will have to login again. To use
# memcached set CACHES to something like
SESSION_ENGINE = &#39;django.contrib.sessions.backends.cache&#39;
CACHES = {
    &#39;default&#39;: {
        &#39;BACKEND&#39;: &#39;django.core.cache.backends.memcached.MemcachedCache&#39;,
        &#39;LOCATION&#39;: &#39;127.0.0.1:11211&#39;,
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;What is that? This is where the sessions are stored. By default, they are not stored on memcached. What is the problem? There is a size limit! It works fine without multi-domain but once you enable it, you can have some problems that are very curious. In our case, before we changed the sessions engine, we could not switch between projects. Let&amp;rsquo;s say user A has access to the projects P1 and P2, he could not change from one to the other. This was &lt;em&gt;not easy&lt;/em&gt; to find, so we are writing it down hoping it will help some people :)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; obvioulsy, we presume that you have memcached installed, which should be the case if you followed the official installation guide for OS Kilo.&lt;/p&gt;

&lt;h2 id=&#34;conclusion:e553667a2781ca6da8f61dd87c8dcbb9&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Enabling a LDAP-backend for authentification is not hard on OpenStack, the problems you may encounter are the creation of the config file for your domain, the fact that you may be using the V2 of Keystone (which does not know about domains at all), the sessions engine not configured on memcached and that&amp;rsquo;s pretty much all. If you followed the guide, it already makes you work with the V3 when it&amp;rsquo;s possible.&lt;/p&gt;

&lt;p&gt;Also, note that when you use the CLI or the API, you also need to specifiy the region, the id of the project&amp;rsquo;s domain and the id of the user&amp;rsquo;s domain (in addition to all the other options that were already asked by the v2 of Keystone, like the username, password, project, etc.).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Openstack Swift</title>
      <link>http://www.heig-cloud.github.io/article/swift/</link>
      <pubDate>Thu, 17 Dec 2015 14:01:53 +0100</pubDate>
      
      <guid>http://www.heig-cloud.github.io/article/swift/</guid>
      <description>

&lt;h2 id=&#34;what-is-openstack-swift:9ba7d4d50c31dd7166688703e242142d&#34;&gt;What is OpenStack Swift&lt;/h2&gt;

&lt;p&gt;Swift is OpenStack&amp;rsquo;s implementation of an object storage. In cloud computing, we often refer to block and object storage. OpenStack provides two different components, each responsibe for one of these parts:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Cinder:&lt;/strong&gt; Block storage&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Swift:&lt;/strong&gt; Object storage&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; We won&amp;rsquo;t be covering Cinder in this article.&lt;/p&gt;

&lt;h2 id=&#34;object-storage-vs-block-storage:9ba7d4d50c31dd7166688703e242142d&#34;&gt;Object storage VS Block storage&lt;/h2&gt;

&lt;p&gt;What is the difference between these two types of storage? Well, the name probably says it all, one is on the block level, it means that you can install an operating system, have a filesystem and use it as you would use any physical drive you may have.&lt;/p&gt;

&lt;p&gt;The other one is about objects, it means you can store data (images, music, videos, archives, all kind of data). So you guessed right, the title is misleading with the &amp;ldquo;VS&amp;rdquo;, because they don&amp;rsquo;t provide the same kind of service.&lt;/p&gt;

&lt;h2 id=&#34;more-about-swift:9ba7d4d50c31dd7166688703e242142d&#34;&gt;More about Swift&lt;/h2&gt;

&lt;p&gt;Here are a few cool things about Swift:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The project is considered &amp;ldquo;mature&amp;rdquo; by the community.&lt;/li&gt;
&lt;li&gt;Quite easy to use.&lt;/li&gt;
&lt;li&gt;Manages the repplication of your objects (3 copys by default)&lt;/li&gt;
&lt;li&gt;Works well with OpenStack (integrated in the dashboard)&lt;/li&gt;
&lt;li&gt;So on&amp;hellip;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;how-do-i-access-data:9ba7d4d50c31dd7166688703e242142d&#34;&gt;How do I access data&lt;/h3&gt;

&lt;p&gt;Swift, like all the components of the stack uses a REST API to communicate. It&amp;rsquo;s also true for the user. It&amp;rsquo;s possible to interact with swift in 3 separate ways:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;CLI:&lt;/strong&gt; using the CLI (Command-Line Interface). To do so, you need to install the python client that provides the command. Usually, the openstack clients packages are named like this: python-*name*client, which in the case of swift is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;python-swiftclient&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;To use it, you only need to install the following packages:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ sudo apt-get install python-swiftclient python-keystoneclient


OR


$ sudo pip install python-swiftclient python-keystoneclient
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Note that we also install the keystone client because by default swift will use keystone for authentification. It&amp;rsquo;s possible to change this, swift has its own auth methods but that would only add some needless complications so we&amp;rsquo;ll stick with keystone.&lt;/p&gt;

&lt;p&gt;Here are a few examples of what you can do with the cli (you need to source an openrc.sh script first or use the env vars):&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ swift -V 3 upload container-name file
$ swift -V 3 download container-name file
$ swift -V 3 post container-name 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;So, in order:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Uploads a file (or a directory) on the given container&lt;/li&gt;
&lt;li&gt;Downloads a file (or a directory) from the given container&lt;/li&gt;
&lt;li&gt;Creates a new container with the given name (if does not exist)&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;Python API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;You can also work on your own version of the python client, we don&amp;rsquo;t do that so this is just for reference. More info can be found &lt;a href=&#34;http://docs.openstack.org/developer/python-swiftclient/swiftclient.html&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;REST API&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Usually, the cli python clients do nothing more than provide you with friendly commands you can use on your terminal. All these commands exist with the REST API as well (the basic rule is, you should be able to do the same things with the CLI and the REST API). As for Swift, we never really had to work with the API, but once again, you can check it &lt;a href=&#34;http://developer.openstack.org/api-ref-objectstorage-v1.html&#34;&gt;here&lt;/a&gt; for reference.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;what-can-i-do-with-an-object-storage:9ba7d4d50c31dd7166688703e242142d&#34;&gt;What can I do with an object storage&lt;/h3&gt;

&lt;p&gt;A fair question indeed. You can either use it for storage, which would probably be the primary option but there are other things one can do with an object store. This image will give you a good hint:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/swift/hadoop_spark_scala.png&#34; alt=&#34;Hadoop, Spark and Scala&#34; /&gt;&lt;/p&gt;

&lt;p&gt;If you have ever worked with Hadoop or Spark, chances are pretty good that you are familiar with HDFS. If not, HDFS is simply a shared filesystem used by the hadoop workers to share data (read and write). Hadoop needs HDFS and Spark may use it as well but is not entirely dependant. It should be possible (as we read on Spark&amp;rsquo;s website) to replace HDFS by Swift. Why would we do that though? Well, first reason, we already have a working swift cluster. An other reason is, to use HDFS we need to install it on all the workers, meaning if we want to have a &amp;ldquo;virtual&amp;rdquo; cluster, we still need to give our instances a lot of &lt;strong&gt;block storage&lt;/strong&gt;. Also, deleting the instances will delete the HDFS cluster, which won&amp;rsquo;t be the case with Swift.&lt;/p&gt;

&lt;p&gt;We are still working on this but the big part of the work is to adapt our codes to work with swift (login, access, etc). We have high expectations regarding this so we&amp;rsquo;ll keep working on it until it works :)&lt;/p&gt;

&lt;h2 id=&#34;problems-with-swift:9ba7d4d50c31dd7166688703e242142d&#34;&gt;Problems with Swift&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve had a few problems while installing Swift. First, swift does not create logs files, it uses syslog for logging. It means that you need to consult this file to see what&amp;rsquo;s up:
&amp;gt; /var/log/syslog&lt;/p&gt;

&lt;p&gt;Is this really a problem? No but it&amp;rsquo;s worth knowing :)&lt;/p&gt;

&lt;p&gt;Also, Swift makes a loooooot of logs, because it&amp;rsquo;s very verbose. Once again, having an ELK Stack is very useful :P (you can check our post about it if you are interested).&lt;/p&gt;

&lt;p&gt;The installation in itself is pretty straightforward, once the conf files are downloaded, change them accordingly, create the rings, distribute them accross your swift-storage nodes and check if everything works fine.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>ELK Stack</title>
      <link>http://www.heig-cloud.github.io/article/elk/</link>
      <pubDate>Wed, 16 Dec 2015 17:11:39 +0100</pubDate>
      
      <guid>http://www.heig-cloud.github.io/article/elk/</guid>
      <description>

&lt;h2 id=&#34;what-is-elk:36c24ba243767c751831e175c3e18b66&#34;&gt;What is ELK?&lt;/h2&gt;

&lt;p&gt;We are talking about Elasticsearch, Logstash and Kibana.&lt;/p&gt;

&lt;h2 id=&#34;what-is-it:36c24ba243767c751831e175c3e18b66&#34;&gt;What is it?&lt;/h2&gt;

&lt;p&gt;This is a very powerful combination of applications that allows you to centralize all of your logs, index them and consult them very easily and efficiently.&lt;/p&gt;

&lt;p&gt;This stack is quite powerful but is not exactly easy to install. We won&amp;rsquo;t show you the whole process but there are a few points of interests that we will try to cover. Stick with us ;-)&lt;/p&gt;

&lt;h3 id=&#34;the-stack:36c24ba243767c751831e175c3e18b66&#34;&gt;The Stack&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/elk/elkstack.png&#34; alt=&#34;ELK Stack&#34; /&gt;&lt;/p&gt;

&lt;p&gt;First of all, let&amp;rsquo;s see what these applications can do.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Elasticsearch&lt;/strong&gt;, is used to index the logs, parse them and apply filters to make efficient requests. This is the node of the stack.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logstash&lt;/strong&gt;, is used to centralize the logs. In the stack, Logstash will collect all the logs and pass them to Elasticsearch. It makes use of logstash-forwarders (we&amp;rsquo;ll come back to this later on).&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Kibana&lt;/strong&gt;
Kibana is the tool used for visualizing the logs. It&amp;rsquo;s a web based interface that is quite easy to use and pretty powerful.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That&amp;rsquo;s it for the stack, of course, this is a very simple and naive way to describe it, but you don&amp;rsquo;t need to know much more than that.&lt;/p&gt;

&lt;h3 id=&#34;why-use-it:36c24ba243767c751831e175c3e18b66&#34;&gt;Why use it&lt;/h3&gt;

&lt;p&gt;As the name implies, OpenStack is a stack of components. It means that all of them can be managed separately. One of the big drawbacks is the fact that every component creates its own logs. Why is this a problem you may ask? Well, let&amp;rsquo;s say we have a problem somewhere, for instance, users can&amp;rsquo;t create new instances. If you are using Horizon as the dashboard, you probably know that the error messages are not always very self-explanatory. You will need to check every log file on every compute nodes (you could have a lot of them), the controller and who knows, maybe even the network node.&lt;/p&gt;

&lt;p&gt;It would be nice if you could just see the messages that present the keyword error for example, right? Well guess what, you can :)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.heig-cloud.github.io/static/img/elk/kibana_preview.png&#34; alt=&#34;Kibana preview&#34; /&gt;&lt;/p&gt;

&lt;p&gt;We can use Kibana to search the logs by specifying the time and a filter. By adding this filter:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;message=error 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;We make sure to only get the logs that contain the word error (which is a simple way to find easy errors). You can filter by host, time, type of the message, etc. There are so much possibilities and we are only scratching the surface there. This should have convinced you to try ELK for your installation.&lt;/p&gt;

&lt;h2 id=&#34;how-do-i-install-it:36c24ba243767c751831e175c3e18b66&#34;&gt;How do I install it&lt;/h2&gt;

&lt;p&gt;We&amp;rsquo;ve been following the tutorial on &lt;a href=&#34;https://www.digitalocean.com/community/tutorials/how-to-install-elasticsearch-logstash-and-kibana-elk-stack-on-ubuntu-14-04&#34;&gt;digitalocean&lt;/a&gt; for our own installation, it&amp;rsquo;s very well made and up to date. We will not provide a step-by-step installation in this post, but we may do it if there is a demand ;-)&lt;/p&gt;

&lt;h2 id=&#34;what-to-be-careful-about:36c24ba243767c751831e175c3e18b66&#34;&gt;What to be careful about&lt;/h2&gt;

&lt;p&gt;As always, the biggest problem we encountered is the fact that we have to modify a lot of different files, on different hosts, so it&amp;rsquo;s quite easy to make a mistake and not so easy to find it. As always, we&amp;rsquo;ve been using Ansible to install the ELK stack and it prevents us from committing a lot of stupid errors and mistakes. In this setup, we have:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;The ELK server&lt;/li&gt;
&lt;li&gt;The other servers (clients let&amp;rsquo;s say)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It means that there are only 2 differents setups. All the nodes except from the controller, which is also the ELK server, have the same configuration. That&amp;rsquo;s why it makes even more sense to automatize the process with Ansible.&lt;/p&gt;

&lt;h3 id=&#34;logstash-conf:36c24ba243767c751831e175c3e18b66&#34;&gt;Logstash conf&lt;/h3&gt;

&lt;p&gt;This is probably the biggest problem you&amp;rsquo;ll encounter while installing the stack. The conf file used by logstash is often &amp;ldquo;cut&amp;rdquo; in multiple parts. It means that you can have 3 or 5 different files to configure your logstash service. The truth is when logstash loads the conf, it takes all the files and just makes a new one by copying in succession the content. Example:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;File A&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file A
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;File B&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file B
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;File C&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;In the end, the conf file will be like this:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Content of file A
Content of file B
Content of file C
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Well, why not? The problem is, if you have twice the same information on different files, let&amp;rsquo;s say a bind command. The following example is used to configure the input sources for logstash:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;02-input-filebeat.conf&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;input {
  beats {
    port =&amp;gt; 5044
    type =&amp;gt; &amp;quot;logs&amp;quot;
    ssl =&amp;gt; true
    ssl_certificate =&amp;gt; &amp;quot;/etc/pki/tls/certs/logstash-forwarder.crt&amp;quot;
    ssl_key =&amp;gt; &amp;quot;/etc/pki/tls/private/logstash-forwarder.key&amp;quot;
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, what happens if by any chance you have this file &lt;strong&gt;TWICE&lt;/strong&gt;. You may wonder why that would happend, well it does sometimes. If by any chance your OS makes a copy of this file (let&amp;rsquo;s say a backup for whatever the reason), you&amp;rsquo;ll have twice the same command. It becomes a problem with a line like this one:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;port =&amp;gt; 5044
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Basically, you will try to bind twice the same port, which will obviously end up with an error. So the morale of the story is:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ALWAYS check in your /etc/logstash/conf.d/ directory that you don&amp;rsquo;t have twice the same file!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This has happened to a lot of people (us too) and can easily make you waste a few hours :(&lt;/p&gt;

&lt;h2 id=&#34;conclusion:36c24ba243767c751831e175c3e18b66&#34;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The ELK Stack is a really good thing to have on your OpenStack cluster to be efficient and find errors that you probably wouldn&amp;rsquo;t find otherwise (or with a lot more effort).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Architecture</title>
      <link>http://www.heig-cloud.github.io/article/architecture/</link>
      <pubDate>Wed, 16 Dec 2015 16:59:49 +0100</pubDate>
      
      <guid>http://www.heig-cloud.github.io/article/architecture/</guid>
      <description>

&lt;h2 id=&#34;architecture-of-our-project:1bf455b3a3c6e1e40be41ef6023b75eb&#34;&gt;Architecture of our project&lt;/h2&gt;

&lt;p&gt;This first post is about the architecture of our OpenStack deployment. This chapter is important because&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>About</title>
      <link>http://www.heig-cloud.github.io/about/</link>
      <pubDate>Wed, 16 Dec 2015 10:59:56 +0100</pubDate>
      
      <guid>http://www.heig-cloud.github.io/about/</guid>
      <description>

&lt;h2 id=&#34;who-we-are:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;Who we are&lt;/h2&gt;

&lt;p&gt;We are a little group of professors and scientific collaborators that has started working on the deployment of an OpenStack private cloud. The aim is to be able to provide our staff with an efficient alternative to Amazon&amp;rsquo;s service AWS. The deployment in itself is also very educative and is a very interesting subject to tackle.&lt;/p&gt;

&lt;h2 id=&#34;why-the-blog:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;Why the blog&lt;/h2&gt;

&lt;p&gt;We wanted to share our little experience with all the others people trying to work with OpenStack. It&amp;rsquo;s not always easy to get stuff working, sometimes the documentation has flaws, is not clear enough, there is also a certain lack of information on the web but that&amp;rsquo;s part of the game. We will be posting some of our results and experiments on this blog, hoping that it may assist those in need :)&lt;/p&gt;

&lt;h2 id=&#34;what-we-did:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;What we did&lt;/h2&gt;

&lt;p&gt;It&amp;rsquo;s a lot of work to install the stack from scratch, it&amp;rsquo;s also very easy to forget something and even easier to make mistakes. That&amp;rsquo;s why we&amp;rsquo;ve decided to implement our solution using an automation tool. The one we chose is Ansible.&lt;/p&gt;

&lt;p&gt;As for the infrastructure, we have 10 servers (there will be a post with more informations about this of course) and we followed the neutron network architecture from &lt;a href=&#34;http://docs.openstack.org/kilo/install-guide/install/apt/content/ch_basic_environment.html#basics-networking-neutron&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&#34;what-can-you-expect-of-us:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;What can you expect of us&lt;/h2&gt;

&lt;p&gt;Well, we will be posting different things on this blog, from basic to more advanced. There won&amp;rsquo;t be a &amp;ldquo;full installation tutorial&amp;rdquo; in here. We will basically write about what we liked and what we think is useful or necessary.&lt;/p&gt;

&lt;h2 id=&#34;i-have-a-question-can-i-contact-you-directly:6083a88ee3411b0d17ce02d738f69d47&#34;&gt;I have a question, can I contact you directly?&lt;/h2&gt;

&lt;p&gt;Every post has a comment section (Disqus API), feel free to ask your questions on there (if it&amp;rsquo;s related to the actual post). If you have a different question that we don&amp;rsquo;t cover in one of our posts, you can contact us here: ADD CONTACT&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>